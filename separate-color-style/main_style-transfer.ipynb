{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6CH_vkV8IEs",
        "outputId": "ccc64d9e-60d8-4718-b4f5-0a4a870b01ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: mps\n"
          ]
        }
      ],
      "source": [
        "import dataset\n",
        "from visualize import *\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "from skimage import color\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from torch import nn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
        "                else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /Users/matteom/miniconda3/envs/torch/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ],
      "source": [
        "import lpips\n",
        "perceptual_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "# d = loss_fn.forward(im0,im1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse_loss_fn = nn.MSELoss()\n",
        "def perceptual_and_MSE_loss(reproduced_image, original_image):\n",
        "    '''\n",
        "    reproduced_image: output of the model\n",
        "    original_image: ground truth\n",
        "    '''\n",
        "    # Perceptual loss\n",
        "    perceptual_loss = perceptual_loss_fn(reproduced_image, original_image).mean()\n",
        "    \n",
        "    # MSE loss\n",
        "    mse_loss = mse_loss_fn(reproduced_image, original_image)\n",
        "    \n",
        "    return perceptual_loss + mse_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_C = 3):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "   \n",
        "        self.conv1_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_C, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.maxpool_1to2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.maxpool_2to3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.maxpool_3to4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "        self.maxpool_4to5 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,content_image):\n",
        "        content_image=content_image.to(device)\n",
        "\n",
        "        # Encoder\n",
        "        conv1 = self.conv1(content_image)\n",
        "\n",
        "        maxpooled_1to2 = self.maxpool_1to2(conv1)\n",
        "        conv2 = self.conv2(maxpooled_1to2)\n",
        "\n",
        "        maxpooled_2to3 = self.maxpool_2to3(conv2)\n",
        "        conv3 = self.conv3(maxpooled_2to3)\n",
        "\n",
        "        maxpooled_3to4 = self.maxpool_3to4(conv3)\n",
        "        conv4 = self.conv4(maxpooled_3to4)\n",
        "\n",
        "        maxpooled_4to5 = self.maxpool_4to5(conv4)\n",
        "        conv5 = self.conv5(maxpooled_4to5)\n",
        "\n",
        "        return conv1, conv2, conv3, conv4, conv5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, out_C=3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.conv_transpose_5to6 = nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1)\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.conv_transpose_6to7 = nn.ConvTranspose2d(256, 256, 4, stride=2, padding=1)\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.conv_transpose_7to8 = nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1)\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv_transpose8to9 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)\n",
        "        self.conv9 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, padding=1),      # Simmetry broken here: keeps being 64 (from paper)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv10 = nn.Conv2d(64, out_C, 1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,latent_features):\n",
        "        # Decoder\n",
        "        conv_transpose_5to6=self.conv_transpose_5to6(latent_features)\n",
        "        conv6 = self.conv6(conv_transpose_5to6)\n",
        "\n",
        "        conv_transpose_6to7 = self.conv_transpose_6to7(conv6)\n",
        "        conv7 = self.conv7(conv_transpose_6to7)\n",
        "\n",
        "        conv_transpose_7to8 = self.conv_transpose_7to8(conv7)\n",
        "        conv8 = self.conv8(conv_transpose_7to8)\n",
        "\n",
        "        concatenation_8_to9 = self.conv_transpose8to9(conv8)\n",
        "        conv9 = self.conv9(concatenation_8_to9)\n",
        "\n",
        "        output = self.conv10(conv9)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_mean_std(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std\n",
        "\n",
        "def adain(content_feat, style_feat):\n",
        "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
        "    size = content_feat.size()\n",
        "    style_mean, style_std = calc_mean_std(style_feat)\n",
        "    content_mean, content_std = calc_mean_std(content_feat)\n",
        "\n",
        "    normalized_feat = (content_feat - content_mean.expand(\n",
        "        size)) / content_std.expand(size)\n",
        "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
        "\n",
        "def adjust_learning_rate(opts, iteration_count, lr,lr_decay):\n",
        "    \"\"\"Imitating the original implementation\"\"\"\n",
        "    lr = lr / (1.0 + lr_decay * iteration_count)\n",
        "    for opt in opts:\n",
        "        for param_group in opt.param_groups:\n",
        "            param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, checkpoint_path=None):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        encoder = Encoder(in_C=3)\n",
        "        if checkpoint_path is not None:\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "            encoder.load_state_dict(checkpoint, strict=False)\n",
        "        self.encoder = encoder.to(device)\n",
        "\n",
        "        self.decoder_L = Decoder(out_C=1).to(device)\n",
        "        self.decoder_ab = Decoder(out_C=2).to(device)\n",
        "    \n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def encode_with_intermediate(self, input):\n",
        "        conv1, conv2, conv3, conv4, conv5 = self.encoder(input)\n",
        "        return [conv1, conv2, conv3, conv4, conv5]\n",
        "\n",
        "    # extract relu4_1 from input image\n",
        "    def encode(self, input):\n",
        "        _, _, _, _, conv5 = self.encoder(input)\n",
        "        return conv5\n",
        "\n",
        "    def calc_content_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        #assert (target.requires_grad is False)\n",
        "        return self.mse_loss(input, target)\n",
        "\n",
        "    def calc_texture_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        #assert (target.requires_grad is False)\n",
        "        input_mean, input_std = calc_mean_std(input)\n",
        "        target_mean, target_std = calc_mean_std(target)\n",
        "        return self.mse_loss(input_mean, target_mean) + \\\n",
        "               self.mse_loss(input_std, target_std)\n",
        "\n",
        "    def ct_t_loss(self, pred_l, content_l, texture_l):\n",
        "        input_feats = self.encode_with_intermediate(pred_l)\n",
        "        target_ct = self.encode(content_l)\n",
        "        target_t = self.encode_with_intermediate(texture_l)\n",
        "\n",
        "        loss_ct = self.calc_content_loss(input_feats[-1], target_ct)\n",
        "        loss_t = self.calc_texture_loss(input_feats[0], target_t[0])\n",
        "        for i in range(1, len(input_feats) - 1):\n",
        "            loss_t += self.calc_texture_loss(input_feats[i], target_t[i])\n",
        "\n",
        "        return loss_ct, loss_t\n",
        "\n",
        "    def cr_loss(self, pred_ab, color_ab):\n",
        "        zero = torch.zeros(pred_ab.shape[0], 1, pred_ab.shape[2], pred_ab.shape[3]).to(device)\n",
        "        pred_ab = torch.cat([zero, pred_ab], dim=1)\n",
        "\n",
        "        input_cr = self.encode_with_intermediate(pred_ab)\n",
        "        target_cr = self.encode_with_intermediate(color_ab)\n",
        "\n",
        "        loss_cr = self.calc_texture_loss(input_cr[0], target_cr[0])\n",
        "        for i in range(1, len(input_cr) - 1):\n",
        "            loss_cr += self.calc_texture_loss(input_cr[i], target_cr[i])\n",
        "\n",
        "        return loss_cr\n",
        "\n",
        "    def run_L_path(self, content_l, texture_l, alpha = 1.0):\n",
        "        ct_l_feat = self.encode(content_l)\n",
        "        t_l_feat = self.encode(texture_l)\n",
        "        o_l_feat = adain(ct_l_feat, t_l_feat)\n",
        "        o_l_feat = alpha *  o_l_feat + (1.0 - alpha) * ct_l_feat\n",
        "        l_pred = self.L_path(o_l_feat)\n",
        "\n",
        "        return l_pred\n",
        "\n",
        "    def run_AB_path(self, content_ab, color_ab, alpha = 1.0):\n",
        "        ct_ab_feat = self.encode(content_ab)\n",
        "        cr_ab_feat = self.encode(color_ab)\n",
        "        o_ab_feat = adain(ct_ab_feat, cr_ab_feat)\n",
        "        o_ab_feat = alpha * o_ab_feat + (1.0 - alpha) * ct_ab_feat\n",
        "        ab_pred = self.AB_path(o_ab_feat)\n",
        "\n",
        "        return ab_pred\n",
        "\n",
        "    def forward(self, content_ab, color_ab, alpha_ab=1.0):\n",
        "       \n",
        "\n",
        "        ct_ab_feat = self.encode(content_ab)\n",
        "        cr_ab_feat = self.encode(color_ab)\n",
        "\n",
        "        o_ab_feat = adain(ct_ab_feat, cr_ab_feat)\n",
        "        o_ab_feat = alpha_ab * o_ab_feat + (1.0 - alpha_ab) * ct_ab_feat\n",
        "\n",
        "        ab_pred = self.decoder_ab(o_ab_feat)\n",
        "\n",
        "        return ab_pred\n",
        "    \n",
        "\n",
        "    def train_model(self, \n",
        "              train_loader, val_loader,\n",
        "              lr=1e-4, lr_decay=1e-5,\n",
        "              epochs=100):\n",
        "\n",
        "        self.train()\n",
        "\n",
        "        # Training options\n",
        "\n",
        "        opt_AB = torch.optim.Adam(self.decoder_ab.parameters(), lr=lr)\n",
        "        opts = [opt_AB]\n",
        "\n",
        "        # For stats purposes\n",
        "        best_val_loss = 99999\n",
        "        loss_archive = {\"training\": [], \"validation\": []}\n",
        "\n",
        "        mb = master_bar(range(epochs))\n",
        "        for epoch in mb:\n",
        "            train_loss = 0\n",
        "\n",
        "            adjust_learning_rate(opts, iteration_count=epoch, lr=lr, lr_decay=lr_decay)\n",
        "\n",
        "            total=len(train_loader)\n",
        "            for i, (context_batch_data, style_batch_data) in progress_bar(enumerate(train_loader), total=total, parent=mb):\n",
        "                content_ab= context_batch_data[:,1:,:,:].to(device)\n",
        "                color_ab= style_batch_data[:,1:,:,:].to(device)\n",
        "\n",
        "                # S2: Forward\n",
        "\n",
        "                ab_pred = self(content_ab, color_ab)\n",
        "\n",
        "                # S3: Calculate loss\n",
        "                loss_cr = self.cr_loss(ab_pred, color_ab)\n",
        "                train_loss+=loss_cr\n",
        "                loss = loss_cr\n",
        "\n",
        "                # S4: Backward\n",
        "\n",
        "                for opt in opts:\n",
        "                    opt.zero_grad()\n",
        "                loss.backward()\n",
        "                for opt in opts:\n",
        "                    opt.step()\n",
        "\n",
        "            # Stats:\n",
        "            train_loss /= len(train_loader)\n",
        "            loss_archive[\"training\"].append(train_loss)      \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kept apart as it must be the SAME for images and styles!\n",
        "BATCH_SIZE = 10\n",
        "RESOLUTION = (128,128)\n",
        "COLORSPACE = 'RGB'\n",
        "TRAIN_SIZE = 60\n",
        "VAL_SIZE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' # Actual images:\\nTRAIN_SIZE = 60\\nVAL_SIZE = 10\\n\\ntrain_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace=COLORSPACE,resolution=RESOLUTION)\\ntrain_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)\\n '"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" # Actual images:\n",
        "TRAIN_SIZE = 60\n",
        "VAL_SIZE = 10\n",
        "\n",
        "train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace=COLORSPACE,resolution=RESOLUTION)\n",
        "train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' # Styles:\\nS_TRAIN_SIZE = 60\\nS_VAL_SIZE = 10\\n\\nstyle_train_data, style_validation_data = dataset.prepare_styles_dataset(train_size=S_TRAIN_SIZE, test_size=S_VAL_SIZE, batch_size=BATCH_SIZE,colorspace=COLORSPACE,resolution=RESOLUTION)\\nstyle_train_loader, style_validation_loader = dataset.prepare_styles_dataloader(style_train_data, style_validation_data, batch_size=BATCH_SIZE) '"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" # Styles:\n",
        "S_TRAIN_SIZE = 60\n",
        "S_VAL_SIZE = 10\n",
        "\n",
        "style_train_data, style_validation_data = dataset.prepare_styles_dataset(train_size=S_TRAIN_SIZE, test_size=S_VAL_SIZE, batch_size=BATCH_SIZE,colorspace=COLORSPACE,resolution=RESOLUTION)\n",
        "style_train_loader, style_validation_loader = dataset.prepare_styles_dataloader(style_train_data, style_validation_data, batch_size=BATCH_SIZE) \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1288235f75834072837c0fb8b58ed44b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully\n"
          ]
        }
      ],
      "source": [
        "\"\"\" class ContextDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        super(ContextDataset, self).__init__()\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]['image']\n",
        "        return image\n",
        "\n",
        "class StylesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        super(StylesDataset, self).__init__()\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]['image']\n",
        "        return image\n",
        "    \n",
        "class StyleTransferTensorDataset(torch.utils.data.TensorDataset):\n",
        "    def __init__(self, dataset1, dataset2, *args, **kwargs):\n",
        "        super(StylesDataset, self).__init__(*args, **kwargs)\n",
        "        self.dataset1 = dataset1\n",
        "        self.dataset2 = dataset2\n",
        "\n",
        "    def __len__(self):\n",
        "        len1 = len(self.dataset1)\n",
        "        len2 = len(self.dataset2)\n",
        "        assert len1 == len2\n",
        "        return len1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image1 = self.dataset1[idx]\n",
        "        image2 = self.dataset2[idx]\n",
        "        return (image1,image2) \"\"\"\n",
        "\n",
        "class StyleTransferDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data1, data2):\n",
        "        super(StyleTransferDataset, self).__init__()\n",
        "        self.data1 = data1\n",
        "        self.data2 = data2\n",
        "\n",
        "    def __len__(self):\n",
        "        len1 = len(self.data1)\n",
        "        len2 = len(self.data2)\n",
        "        if len1<len2:\n",
        "            return len1\n",
        "        else:\n",
        "            return len2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image1 = self.data1[idx]['image']\n",
        "        image2 = self.data2[idx]['image']\n",
        "        return (image1,image2)\n",
        "\n",
        "\n",
        "def prepare_joint_dataloaders(train_size=10,test_size=10,batch_size=4,colorspace='RGB',resolution=(128,128)):\n",
        "    context_train, context_test = dataset.prepare_dataset(train_size,test_size,batch_size,colorspace,resolution)\n",
        "    styles_train, styles_test = dataset.prepare_styles_dataset(train_size,test_size,batch_size,colorspace,resolution)\n",
        "    # Filter context images\n",
        "    filtered_context_train_data = []\n",
        "    filtered_context_test_data = []\n",
        "    for entry in list(context_train):\n",
        "        if entry['image'].shape[0] == 3:\n",
        "            filtered_context_train_data.append(entry)\n",
        "    for entry in list(context_test):\n",
        "        if entry['image'].shape[0] == 3:\n",
        "            filtered_context_test_data.append(entry)\n",
        "    # Filter styles images\n",
        "    filtered_styles_train_data = []\n",
        "    filtered_styles_test_data = []\n",
        "    for entry in styles_train:\n",
        "        if entry['image'].shape[0] == 3:\n",
        "            filtered_styles_train_data.append(entry)\n",
        "    for entry in styles_test:\n",
        "        if entry['image'].shape[0] == 3:\n",
        "            filtered_styles_test_data.append(entry)\n",
        "    # Datasets\n",
        "    train_dataset = StyleTransferDataset(filtered_context_train_data, filtered_styles_train_data)\n",
        "    test_dataset = StyleTransferDataset(filtered_context_test_data, filtered_styles_test_data)\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "train_dataloader, test_dataloader = prepare_joint_dataloaders(TRAIN_SIZE,VAL_SIZE,BATCH_SIZE,COLORSPACE,RESOLUTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is:  mps\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#model = load_model_from_checkpoint(\"./colorisation/UNetREG_trsize50_valsize5nope_best_model.pth.tar\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_model(train_loader\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_loader\u001b[49m, val_loader\u001b[38;5;241m=\u001b[39mvalidation_loader, \n\u001b[1;32m      8\u001b[0m                         style_train_loader\u001b[38;5;241m=\u001b[39mstyle_train_loader, style_val_loader\u001b[38;5;241m=\u001b[39mstyle_validation_loader,\n\u001b[1;32m      9\u001b[0m                         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, \n\u001b[1;32m     10\u001b[0m                         \u001b[38;5;66;03m#save_path= 'style-transfer', \u001b[39;00m\n\u001b[1;32m     11\u001b[0m                         \u001b[38;5;66;03m#save_name_prefix='/AB_trsize60-60',\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                         \u001b[38;5;66;03m#val_check_every=5,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m                         \u001b[38;5;66;03m#plot_every=15,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m                         \u001b[38;5;66;03m#plotting_samples=list(validation_data)[:3]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m                         )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Device is: \", device)\n",
        "model = Net(checkpoint_path=\"./colorisation/UNetREG_trsize1000_valsize100_best_model.pth.tar\").to(device)\n",
        "\n",
        "#model = load_model_from_checkpoint(\"./colorisation/UNetREG_trsize50_valsize5nope_best_model.pth.tar\")\n",
        "model = model.to(device)\n",
        "\n",
        "model.train_model(train_loader=train_dataloader, val_loader=test_dataloader, \n",
        "                        epochs=300, lr=1e-4, \n",
        "                        #save_path= 'style-transfer', \n",
        "                        #save_name_prefix='/AB_trsize60-60',\n",
        "                        #val_check_every=5,\n",
        "                        #plot_every=15,\n",
        "                        #plotting_samples=list(validation_data)[:3]\n",
        "                        )\n",
        "\n",
        "   "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
