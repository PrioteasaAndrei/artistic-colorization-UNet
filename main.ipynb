{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prio/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Working on device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))  # 0 corresponds to the first GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on device: \", device)\n",
    "\n",
    "TRAIN_SIZE = 10\n",
    "TEST_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prio/miniconda3/lib/python3.12/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "/home/prio/miniconda3/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for imagenet-1k contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/imagenet-1k\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/prio/miniconda3/lib/python3.12/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "/home/prio/miniconda3/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for imagenet-1k contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/imagenet-1k\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/prio/Documents/artistic-colorization-UNet/dataset.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(gray_img)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All tests passed'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n",
      "Data loader prepared successfully\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = prepare_dataset(TRAIN_SIZE,TEST_SIZE)\n",
    "train_loader, test_loader = prepare_dataloader(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encoder is a pretrained VGG up to relu4_1 as in the original paper (see 6.1 paper)\n",
    "'''\n",
    "class VGG_Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_Encoder, self).__init__()\n",
    "        pretrained = torchvision.models.vgg19(pretrained=True)\n",
    "        \n",
    "        f = torch.nn.Sequential(*list(pretrained.features.children())[:21]).eval()\n",
    "\n",
    "        ## adding an extra conv layer because we have 1 channel images\n",
    "        self.adjuster = torch.nn.Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        \n",
    "        # Splitting the network so we can get output of different layers\n",
    "        # TODO: ADD REFLECTION PADDING LAYERS\n",
    "        self.relu1_1 = torch.nn.Sequential(*f[:2],)\n",
    "        self.relu2_1 = torch.nn.Sequential(*f[2:5], *f[5:7])\n",
    "        self.relu3_1 = torch.nn.Sequential(*f[7:10],*f[10:12])\n",
    "        self.relu4_1 = torch.nn.Sequential(*f[12:14],\n",
    "                                          *f[14:16],\n",
    "                                          *f[16:19],\n",
    "                                           *f[19:21])\n",
    "        \n",
    "        for param in self.relu1_1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.relu2_1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.relu3_1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.relu4_1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.adjuster(x)\n",
    "        out_1 = self.relu1_1(x)\n",
    "        out_2 = self.relu2_1(out_1)\n",
    "        out_3 = self.relu3_1(out_2)\n",
    "        result = self.relu4_1(out_3)\n",
    "        return out_1, out_2, out_3, result\n",
    "\n",
    "def mean_and_std(x):\n",
    "    x = x.view(x.shape[0], x.shape[1], -1)\n",
    "    mean = x.mean(dim=2) + 0.00005\n",
    "    std = x.var(dim=2).sqrt()\n",
    "    return mean.view(mean.shape[0], mean.shape[1], 1, 1), std.view(std.shape[0], std.shape[1], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prio/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prio/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG_Encoder(\n",
      "  (adjuster): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1_1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (relu2_1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "  )\n",
      "  (relu3_1): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "  )\n",
      "  (relu4_1): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = VGG_Encoder()\n",
    "\n",
    "# print(encoder.adjuster(torch.rand(4,1,256,256)).shape)\n",
    "\n",
    "print(encoder)\n",
    "out_1,out_2,out_3,out_4= encoder(torch.rand(4,1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 256, 256]) torch.Size([4, 128, 128, 128]) torch.Size([4, 256, 64, 64]) torch.Size([4, 512, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(out_1.shape,out_2.shape,out_3.shape,out_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://medium.com/analytics-vidhya/unet-implementation-in-pytorch-idiot-developer-da40d955f201\n",
    "'''\n",
    "\n",
    "class conv_block(torch.nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(out_c)        \n",
    "        self.conv2 = torch.nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(out_c)         \n",
    "        self.relu = torch.nn.ReLU()     \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)        \n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class decoder_block(torch.nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)     \n",
    "        \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        print(\"shape of inputs\",inputs.shape,\"shape of skip\",skip.shape)  \n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Unet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = VGG_Encoder()\n",
    "\n",
    "\n",
    "        ## ----------------- v1 ----------------\n",
    "\n",
    "        ## Bottleneck\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "        \n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        ## ----------------- v2 ----------------\n",
    "        # \"\"\" Decoder \"\"\"\n",
    "        # self.d1 = decoder_block(512, 256)\n",
    "        # self.d2 = decoder_block(256, 128)\n",
    "        # self.d3 = decoder_block(128, 64)\n",
    "        # self.d4 = decoder_block(64, 32)\n",
    "\n",
    "\n",
    "        ## output should be 3 channels image\n",
    "        self.out = torch.nn.Conv2d(64, 3, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Encoder\n",
    "        out1, out2, out3, out4 = self.encoder(x)\n",
    "        \n",
    "        ## Decoder\n",
    "        \n",
    "        d1 = self.d1(b, out4)\n",
    "        d2 = self.d2(d1, out3)\n",
    "        d3 = self.d3(d2, out2)\n",
    "        d4 = self.d4(d3, out1)\n",
    "        \n",
    "        out = self.out(d4)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prio/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prio/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputs torch.Size([4, 512, 32, 32]) shape of skip torch.Size([4, 512, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 64 but got size 32 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m X \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrayscale_image\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 74\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m out1, out2, out3, out4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# b = self.bottleneck(out4)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m## Decoder\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m## TODO: put b instead of out4\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m d2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md2(d1, out3)\n\u001b[1;32m     76\u001b[0m d3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md3(d2, out2)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m, in \u001b[0;36mdecoder_block.forward\u001b[0;34m(self, inputs, skip)\u001b[0m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(inputs)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape of inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m,inputs\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape of skip\u001b[39m\u001b[38;5;124m\"\u001b[39m,skip\u001b[38;5;241m.\u001b[39mshape)  \n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 64 but got size 32 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "model = Unet().to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        X = batch['grayscale_image'].to(device)\n",
    "        y = batch['image'].to(device)\n",
    "\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' \n",
    "# decoder is just the second part of an Unet\n",
    "# implement skip connections (feed concat to the upsample layer)\n",
    "# '''\n",
    "# class Decoder(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         ## TODO: adapt block one for single channel input\n",
    "#         self.block1 = torch.nn.Sequential(\n",
    "#             torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#             torch.nn.Conv2d(512, 256, (3, 3)),\n",
    "#             torch.nn.ReLU())\n",
    "#         self.block2 = torch.nn.Sequential(\n",
    "#             torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "#             torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#             torch.nn.Conv2d(256, 256, (3, 3)),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#             torch.nn.Conv2d(256, 256, (3, 3)),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#             torch.nn.Conv2d(256, 256, (3, 3)),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#             torch.nn.Conv2d(256, 128, (3, 3)),\n",
    "#             torch.nn.ReLU(),\n",
    "#         )\n",
    "#         self.block3 = torch.nn.Sequential(\n",
    "#             torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "#             torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#             torch.nn.Conv2d(128, 128, (3, 3)),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#             torch.nn.Conv2d(128, 64, (3, 3)),\n",
    "#             torch.nn.ReLU(),\n",
    "#         )\n",
    "#         self.block4 = torch.nn.Sequential(\n",
    "#             torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "#             torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#             torch.nn.Conv2d(64, 64, (3, 3)),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#             torch.nn.Conv2d(64, 3, (3, 3)),\n",
    "#         )\n",
    "\n",
    "\n",
    "#         # self.decode = torch.nn.Sequential(\n",
    "#         #     # torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#         #     # torch.nn.Conv2d(512, 256, (3, 3)),\n",
    "#         #     # torch.nn.ReLU(),\n",
    "#         #     # torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "#         #     # torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#         #     # torch.nn.Conv2d(256, 256, (3, 3)),\n",
    "#         #     # torch.nn.ReLU(),\n",
    "#         #     # torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#         #     # torch.nn.Conv2d(256, 256, (3, 3)),\n",
    "#         #     # torch.nn.ReLU(),\n",
    "#         #     # torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#         #     # torch.nn.Conv2d(256, 256, (3, 3)),\n",
    "#         #     # torch.nn.ReLU(),\n",
    "#         #     # torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#         #     # torch.nn.Conv2d(256, 128, (3, 3)),\n",
    "#         #     # torch.nn.ReLU(),\n",
    "#         #     # torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "#         #     # torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#         #     # torch.nn.Conv2d(128, 128, (3, 3)),\n",
    "#         #     # torch.nn.ReLU(),\n",
    "#         #     # torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#         #     # torch.nn.Conv2d(128, 64, (3, 3)),\n",
    "#         #     # torch.nn.ReLU(),\n",
    "#         #     # torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "#         #     # torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#         #     # torch.nn.Conv2d(64, 64, (3, 3)),\n",
    "#         #     # torch.nn.ReLU(),\n",
    "#         #     # torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "#         #     # torch.nn.Conv2d(64, 3, (3, 3)),\n",
    "#         # )\n",
    "#     def forward(self, x, skips):\n",
    "#         '''\n",
    "#         skips is a list of tensors from the encoder\n",
    "#         concatenation might not work because we have different depth channels\n",
    "#         a 1x1 convolution is usually applied after that as a fusion mechanism\n",
    "#         '''\n",
    "#         out1 = self.block1(x)\n",
    "\n",
    "#         ## skip connection\n",
    "#         out2 = self.block2(torch.cat([out1,x], axis=1))\n",
    "#         out3 = self.block3(torch.cat([out2,x], axis=1))\n",
    "        \n",
    "#         return result\n",
    "# \"\"\"\n",
    "# decode = Decoder()\n",
    "# img = decode(t)\n",
    "# concat_img((img[:12]).detach().cpu())\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try adain before skip connections or without to see if it makes a difference\n",
    "encoder = VGG_Encoder()\n",
    "decoder = Decoder()\n",
    "## generate random tensor of at least 4 dimensions\n",
    "random_tensor = torch.rand((12, 16, 26, 26))\n",
    "style_image  = torch.rand((12, 16, 26, 26))\n",
    "adain = AdaIN()\n",
    "random_tensor = adain(random_tensor, style_image)\n",
    "print(random_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.relu1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoder(torch.rand(1, 3, 256, 256))\n",
    "print(x[3].shape)\n",
    "\n",
    "through_adain = adain(x[3], x[3])\n",
    "\n",
    "output = decoder(through_adain)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AdaIN implementation\n",
    "## TODO: see if the output size is the same as input size\n",
    "class AdaIN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "        self.IN = torch.nn.InstanceNorm2d(512)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        size = x.size()\n",
    "        \n",
    "        x = self.IN(x)\n",
    "        \n",
    "        #mean_x, std_x = mean_and_std(x)\n",
    "        mean_y, std_y = mean_and_std(y)\n",
    "        #x = (x - mean_x.expand(size)) / std_x.expand(size)\n",
    "        x = x * std_y.expand(size) + mean_y.expand(size)\n",
    "        return x\n",
    "\"\"\"\"\n",
    "print(style.shape)\n",
    "mean, std = mean_and_std(style)\n",
    "print(mean.shape)\n",
    "print(std.shape)\n",
    "Ada = AdaIN()\n",
    "t = Ada(vgg(content)[3], vgg(style)[3])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Conv2d(5, 20, kernel_size=1)\n",
    "len(a.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
