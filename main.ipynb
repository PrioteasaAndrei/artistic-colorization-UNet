{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prio/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Working on device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))  # 0 corresponds to the first GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on device: \", device)\n",
    "\n",
    "TRAIN_SIZE = 100\n",
    "TEST_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prio/miniconda3/lib/python3.12/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "/home/prio/miniconda3/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for imagenet-1k contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/imagenet-1k\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/prio/miniconda3/lib/python3.12/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "/home/prio/miniconda3/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for imagenet-1k contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/imagenet-1k\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/prio/Documents/artistic-colorization-UNet/dataset.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(gray_img)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All tests passed'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n",
      "Data loader prepared successfully\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = prepare_dataset(TRAIN_SIZE,TEST_SIZE)\n",
    "train_loader, test_loader = prepare_dataloader(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encoder is a pretrained VGG up to relu4_1 as in the original paper (see 6.1 paper)\n",
    "'''\n",
    "class VGG_Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_Encoder, self).__init__()\n",
    "        pretrained = torchvision.models.vgg19(pretrained=True)\n",
    "        \n",
    "        f = torch.nn.Sequential(*list(pretrained.features.children())[:21]).eval()\n",
    "\n",
    "        # Splitting the network so we can get output of different layers\n",
    "        # TODO: ADD REFLECTION PADDING LAYERS\n",
    "        self.relu1_1 = torch.nn.Sequential(*f[:2],)\n",
    "        self.relu2_1 = torch.nn.Sequential(*f[2:5], *f[5:7])\n",
    "        self.relu3_1 = torch.nn.Sequential(*f[7:10],*f[10:12])\n",
    "        self.relu4_1 = torch.nn.Sequential(*f[12:14],\n",
    "                                          *f[14:16],\n",
    "                                          *f[16:19],\n",
    "                                           *f[19:21])\n",
    "        \n",
    "        for param in self.relu1_1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.relu2_1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.relu3_1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.relu4_1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = self.relu1_1(x)\n",
    "        out_2 = self.relu2_1(out_1)\n",
    "        out_3 = self.relu3_1(out_2)\n",
    "        result = self.relu4_1(out_3)\n",
    "        return out_1, out_2, out_3, result\n",
    "\n",
    "def mean_and_std(x):\n",
    "    x = x.view(x.shape[0], x.shape[1], -1)\n",
    "    mean = x.mean(dim=2) + 0.00005\n",
    "    std = x.var(dim=2).sqrt()\n",
    "    return mean.view(mean.shape[0], mean.shape[1], 1, 1), std.view(std.shape[0], std.shape[1], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VGG_Encoder()\n",
    "print(encoder)\n",
    "out_1,out_2,out_3,out_4= encoder(torch.rand(4,3,256,256))\n",
    "print(out_1.shape,out_2.shape,out_3.shape,out_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://medium.com/analytics-vidhya/unet-implementation-in-pytorch-idiot-developer-da40d955f201\n",
    "'''\n",
    "\n",
    "class conv_block(torch.nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(out_c)        \n",
    "        self.conv2 = torch.nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(out_c)         \n",
    "        self.relu = torch.nn.ReLU()     \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)        \n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class decoder_block(torch.nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)     \n",
    "        \n",
    "    def forward(self, inputs, skip):\n",
    "        display(\"shape of inputs\",inputs.shape,\"shape of skip\",skip.shape)  \n",
    "        x = self.up(inputs)\n",
    "        # x = torch.cat([x, skip], axis=1)\n",
    "        display(\"Concatenation successful\",x.shape)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Unet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = VGG_Encoder()\n",
    "\n",
    "\n",
    "        ## ----------------- v1 ----------------\n",
    "\n",
    "        # ## Bottleneck\n",
    "        # self.bottleneck = conv_block(512, 1024)\n",
    "        \n",
    "        # \"\"\" Decoder \"\"\"\n",
    "        # self.d1 = decoder_block(1024, 512)\n",
    "        # self.d2 = decoder_block(512, 256)\n",
    "        # self.d3 = decoder_block(256, 128)\n",
    "        # self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        ## ----------------- v2 ----------------\n",
    "        # \"\"\" Decoder \"\"\"\n",
    "        self.d0 = decoder_block(512, 512)\n",
    "        self.d1 = decoder_block(512, 256)\n",
    "        self.d2 = decoder_block(256, 128)\n",
    "        self.d3 = decoder_block(128, 64)\n",
    "        self.d4 = decoder_block(64, 32)\n",
    "\n",
    "\n",
    "        ## output should be 3 channels image\n",
    "        self.out = torch.nn.Conv2d(64, 3, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        ## Encoder\n",
    "        out1, out2, out3, out4 = self.encoder(x)\n",
    "\n",
    "        # x = self.d0(x, out4)\n",
    "        # x = self.d1(x, out3)\n",
    "        # x = self.d2(x, out2)\n",
    "        # x = self.d3(x, out1)\n",
    "        # x = self.d4(x, x)\n",
    "\n",
    "        x = self.d0(out4, out4)\n",
    "        x = self.d1(x, out3)\n",
    "        x = self.d2(x, out2)\n",
    "        x = self.d3(x, out1)\n",
    "        x = self.d4(x, out1)\n",
    "\n",
    "        return self.out(x)\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     ## Encoder\n",
    "    #     out1, out2, out3, out4 = self.encoder(x)\n",
    "\n",
    "    #     ## bottleneck\n",
    "    #     out = self.bottleneck(out4)\n",
    "\n",
    "    #     ## decoder\n",
    "    #     d1 = self.d1(out, out4)\n",
    "    #     d2 = self.d2(d1, out3)\n",
    "    #     d3 = self.d3(d2, out2)\n",
    "    #     d4 = self.d4(d3, out1)\n",
    "                \n",
    "        \n",
    "    #     out = self.out(d4)\n",
    "    #     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4,3,256,256)\n",
    "b = torch.rand(4,5,256,256)\n",
    "con = torch.cat([a,b], axis=1)\n",
    "print(con.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet().to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "         \n",
    "        grayscale_X = batch['grayscale_image']\n",
    "        X = grayscale_X.repeat(1,3,1,1).to(device)\n",
    "        y = batch['image'].to(device)\n",
    "\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "decoder is just the second part of an Unet\n",
    "implement skip connections (feed concat to the upsample layer)\n",
    "'''\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decode = torch.nn.Sequential(\n",
    "            torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "            torch.nn.Conv2d(512, 256, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "            torch.nn.Conv2d(256, 256, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "            torch.nn.Conv2d(256, 256, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "            torch.nn.Conv2d(256, 256, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "            torch.nn.Conv2d(256, 128, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "            torch.nn.Conv2d(128, 128, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "            torch.nn.Conv2d(128, 64, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "            torch.nn.Conv2d(64, 64, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "            torch.nn.Conv2d(64, 3, (3, 3)),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decode(x)\n",
    "    \n",
    "class Unet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        \n",
    "        self.encoder = VGG_Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_1, out_2, out_3, out_4 = self.encoder(x)\n",
    "        return self.decoder(out_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prio/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prio/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.6611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.1079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 0.0978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 0.0967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 0.0924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss: 0.0647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss: 0.0672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 loss: 0.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 loss: 0.0668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 loss: 0.0577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 loss: 0.0583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 loss: 0.0611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 loss: 0.0659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 loss: 0.0574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 loss: 0.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [00:02<00:01,  6.39it/s]"
     ]
    }
   ],
   "source": [
    "model = Unet().to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "         \n",
    "        grayscale_X = batch['grayscale_image']\n",
    "        X = grayscale_X.repeat(1,3,1,1).to(device)\n",
    "        y = batch['image'].to(device)\n",
    "\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = list(train_dataset)[5]['image']\n",
    "lab2rgb = LABtoRGB()\n",
    "rgb_image = lab2rgb(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(test_image.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_rgb_image(rgb_tensor):\n",
    "    # Convert the tensor to a numpy array\n",
    "    rgb_array = rgb_tensor.numpy()\n",
    "    \n",
    "    # Transpose the array to match the required format for displaying using PIL\n",
    "    rgb_array = np.transpose(rgb_array, (1, 2, 0))\n",
    "    \n",
    "    # Ensure the data type is uint8 and scale values to [0, 255]\n",
    "    rgb_array = (rgb_array * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert the numpy array to a PIL Image\n",
    "    img = Image.fromarray(rgb_array)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(test_image.shape)\n",
    "print(rgb_image.shape)\n",
    "# Assuming rgb_image_tensor is your tensor of shape [3, 256, 256]\n",
    "display_rgb_image(rgb_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AdaIN implementation\n",
    "## TODO: see if the output size is the same as input size\n",
    "class AdaIN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "        self.IN = torch.nn.InstanceNorm2d(512)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        size = x.size()\n",
    "        \n",
    "        x = self.IN(x)\n",
    "        \n",
    "        #mean_x, std_x = mean_and_std(x)\n",
    "        mean_y, std_y = mean_and_std(y)\n",
    "        #x = (x - mean_x.expand(size)) / std_x.expand(size)\n",
    "        x = x * std_y.expand(size) + mean_y.expand(size)\n",
    "        return x\n",
    "\"\"\"\"\n",
    "print(style.shape)\n",
    "mean, std = mean_and_std(style)\n",
    "print(mean.shape)\n",
    "print(std.shape)\n",
    "Ada = AdaIN()\n",
    "t = Ada(vgg(content)[3], vgg(style)[3])\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
