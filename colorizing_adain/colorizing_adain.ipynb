{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "with open(\"cfg.yaml\", \"r\") as file:\n",
    "        cfg = yaml.safe_load(file)\n",
    "\n",
    "sys.path.append(\"src/\")\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"cuda is available: {torch.cuda.is_available()}\")\n",
    "print(f\"mps is available: {torch.backends.mps.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "\n",
    "import colorization_dataset\n",
    "from net import VGG_Encoder, Decoder, Net, Net_with_LabVGG\n",
    "from train_and_test import train, test_model\n",
    "# from inference_generation import test_transform, style_transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Dataloader and color spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = colorization_dataset.prepare_dataset(train_size=cfg['train_size'], test_size=cfg['test_size'])\n",
    "train_loader, _ = colorization_dataset.prepare_dataloader(train_data, test_data, batch_size=cfg['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_lab= next(iter(train_loader))[\"image\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_image_lab[0].min(), train_image_lab[0].max())\n",
    "print(train_image_lab[1].min(), train_image_lab[1].max())\n",
    "print(train_image_lab[2].min(), train_image_lab[2].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_image_lab.permute(1,2,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_image_lab.permute(1,2,0).numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labtorgb= colorization_dataset.LABtoRGB()\n",
    "train_image_rgb= labtorgb(train_image_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_image_rgb[0].min(), train_image_rgb[0].max())\n",
    "print(train_image_rgb[1].min(), train_image_rgb[1].max())\n",
    "print(train_image_rgb[2].min(), train_image_rgb[2].max())\n",
    "\n",
    "print(train_image_rgb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_image_rgb.permute(1,2,0).numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Network with CIE Lab VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = VGG_Encoder()\n",
    "decoder = Decoder()\n",
    "net= Net_with_LabVGG(encoder, decoder)\n",
    "#net.encoder.load_weights(\"./model/LabVGG16_dict.pth\")\n",
    "net.encoder.load_state_dict(torch.load(\"./model/model_storage/encoder_sigmoid_iter_10.pth.tar\"))\n",
    "net.decoder.load_state_dict(torch.load(\"./model/model_storage/decoder_sigmoid_iter_10.pth.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Using device: mps"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'All tests passed'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n",
      "Data loader prepared successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 10 loss: 0.017226193007081746\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.058396123349666595 max: 0.9258870482444763"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 20 loss: 0.01630485989153385\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.07943324744701385 max: 0.9553374648094177"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 30 loss: 0.016914192214608194\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.042884066700935364 max: 0.9180112481117249"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:59<1:37:28, 59.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 10 loss: 0.01682488275691867\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.01741863042116165 max: 0.9518933892250061"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 20 loss: 0.01646793857216835\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.027927810326218605 max: 0.9549773335456848"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 30 loss: 0.015575944818556309\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.03939688578248024 max: 0.9614298343658447"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [01:56<1:34:32, 57.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 10 loss: 0.016397522389888765\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.03585488721728325 max: 0.9552483558654785"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 20 loss: 0.015812557470053435\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.022334378212690353 max: 0.9857432246208191"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 30 loss: 0.015472946409136056\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.01677960343658924 max: 0.9791161417961121"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [02:53<1:33:31, 57.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 10 loss: 0.016103442758321762\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.02337886020541191 max: 0.9726141095161438"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 20 loss: 0.015184355527162552\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.014796859584748745 max: 0.9647319912910461"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 30 loss: 0.014790221489965916\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "reproduced_image min: 0.01307931262999773 max: 0.9708022475242615"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [03:51<1:32:21, 57.72s/it]"
     ]
    }
   ],
   "source": [
    "train(net,cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = colorization_dataset.prepare_dataset(train_size=cfg['train_size'], test_size=cfg['test_size'])\n",
    "train_loader, _ = colorization_dataset.prepare_dataloader(train_data, test_data, batch_size=cfg['batch_size'])\n",
    "input= next(iter(train_loader))[\"image\"][5]\n",
    "plt.imshow(input.permute(1,2,0).numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=net.to(device)\n",
    "if len(input.shape)==3:\n",
    "    input=input.unsqueeze(0)\n",
    "input=input.to(device)\n",
    "print(input.shape)\n",
    "encoded= net.encoder(input)\n",
    "print(encoded.shape)\n",
    "decoded=decoder(encoded)\n",
    "print(decoded.shape)\n",
    "\n",
    "decoded=decoded.cpu()\n",
    "# plt.imshow(decoded.detach().numpy().squeeze().permute(1,2,0).numpy())\n",
    "plt.imshow(decoded.squeeze(0).permute(1,2,0).detach().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing load of encoder weights back in the network (NOT UP TO DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabVGG16()\n",
    "decoder = Decoder()\n",
    "#decoder.load_state_dict(torch.load(cfg[\"decoder\"]))\n",
    "net= Net(encoder, decoder)\n",
    "net.enc_1.load_state_dict(torch.load(cfg[\"enc_1\"]))\n",
    "net.enc_2.load_state_dict(torch.load(cfg[\"enc_2\"]))\n",
    "net.enc_3.load_state_dict(torch.load(cfg[\"enc_3\"]))\n",
    "net.enc_4.load_state_dict(torch.load(cfg[\"enc_4\"]))\n",
    "net.decoder.load_state_dict(torch.load(cfg[\"decoder\"]))\n",
    "\n",
    "test_model(net, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference test on AdaIN (NOT UP TO DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the image and the style I want to mix\n",
    "# --> Set their path in cfg.yaml\n",
    "input_img=Image.open(\"data/content_dir/brad_pitt.jpg\")\n",
    "display(input_img)\n",
    "style_ref=Image.open(\"data/style_dir/brushstrokes.jpg\")\n",
    "display(style_ref)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "\n",
    "output_dir = Path(cfg[\"output_dir\"])\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Either --content or --contentDir should be given.\n",
    "assert (cfg[\"content\"] or cfg[\"content_dir\"])\n",
    "if cfg[\"content\"]:\n",
    "    content_paths = [Path(cfg[\"content\"])]\n",
    "else:\n",
    "    content_dir = Path(cfg[\"content_dir\"])\n",
    "    content_paths = [f for f in content_dir.glob('*')]\n",
    "\n",
    "# Either --style or --styleDir should be given.\n",
    "assert (cfg[\"style\"] or cfg[\"style_dir\"])\n",
    "if cfg[\"style\"]:\n",
    "    style_paths = cfg[\"style\"].split(',')\n",
    "    if len(style_paths) == 1:\n",
    "        style_paths = [Path(cfg[\"style\"])]\n",
    "    else:\n",
    "        do_interpolation = True\n",
    "        assert (cfg[\"style_interpolation_weights\"] != ''), \\\n",
    "            'Please specify interpolation weights'\n",
    "        weights = [int(i) for i in cfg[\"style_interpolation_weights\"].split(',')]\n",
    "        interpolation_weights = [w / sum(weights) for w in weights]\n",
    "else:\n",
    "    style_dir = Path(cfg[\"style_dir\"])\n",
    "    style_paths = [f for f in style_dir.glob('*')]\n",
    "\n",
    "encoder = VGG_Encoder()\n",
    "decoder = Decoder()\n",
    "\n",
    "decoder.eval()\n",
    "encoder.eval()\n",
    "\n",
    "decoder.load_state_dict(torch.load(cfg[\"decoder\"]))\n",
    "# encoder.load_state_dict(torch.load(args.vgg))\n",
    "#vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "content_tf = test_transform(cfg[\"content_size\"], cfg[\"crop\"])\n",
    "style_tf = test_transform(cfg[\"style_size\"], cfg[\"crop\"])\n",
    "\n",
    "do_interpolation = False\n",
    "\n",
    "for content_path in content_paths:\n",
    "\n",
    "    # |||||||\n",
    "    # not in use\n",
    "    # |||||||\n",
    "    # vvvvvvv\n",
    "    if do_interpolation:  # one content image, N style image\n",
    "        style = torch.stack([style_tf(Image.open(str(p))) for p in style_paths])\n",
    "        content = content_tf(Image.open(str(content_path))) \\\n",
    "            .unsqueeze(0).expand_as(style)\n",
    "        style = style.to(device)\n",
    "        content = content.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = style_transfer(encoder, decoder, content, style,\n",
    "                                    cfg[\"alpha\"], interpolation_weights)\n",
    "        output = output.cpu()\n",
    "        output_name = output_dir / '{:s}_interpolation{:s}'.format(\n",
    "            content_path.stem, cfg[\"save_ext\"])\n",
    "        torchvision.utils.save_image(output, str(output_name))\n",
    "    # ^^^^^^^\n",
    "    # |||||||\n",
    "    # not in use\n",
    "    # |||||||\n",
    "        \n",
    "\n",
    "    else:  # process one content and one style\n",
    "        for style_path in style_paths:\n",
    "            content = content_tf(Image.open(str(content_path)))\n",
    "            style = style_tf(Image.open(str(style_path)))\n",
    "            #if cfg[\"preserve_color\"]:\n",
    "            #    style = coral(style, content)\n",
    "            style = style.to(device).unsqueeze(0)\n",
    "            content = content.to(device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                output = style_transfer(encoder, decoder, content, style,\n",
    "                                        cfg[\"alpha\"])\n",
    "            output = output.cpu()\n",
    "            output_name = output_dir / '{:s}_stylized_{:s}{:s}'.format(\n",
    "                content_path.stem, style_path.stem, cfg[\"save_ext\"])\n",
    "            torchvision.utils.save_image(output, str(output_name))\n",
    "\n",
    "output_image=Image.open(\"data/output_dir/brad_pitt_stylized_brushstrokes.jpg\")\n",
    "display(output_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
