{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2205.12867.pdf\n",
    "\n",
    "Color space: La*b*\n",
    "\n",
    "Input to model: L channel so HxW\n",
    "L, a* and b* (-128,127) need to be normalized to 0-1. After propagation, a*, b* output channel value has to be back to 0 - 1 range with sigmoid function. \n",
    "\n",
    "From the paper: U-Net took up an excess of 200 GB in memory when only using 64x64 size of an image. We used 682.382 images for training, 13700 images as data for validation, and 1370 images for the test, consisting of 137 total classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEIGHT=224\n",
    "# WIDTH=224\n",
    "# ImagePath=\"../input/dataset/dataset_updated/training_set/painting/\"\n",
    "\n",
    "# def ExtractInput(path):\n",
    "#     X_img=[]\n",
    "#     y_img=[]\n",
    "#     for imageDir in os.listdir(ImagePath):\n",
    "#         try:\n",
    "#             img = cv2.imread(ImagePath + imageDir)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "#             img = img.astype(np.float32)\n",
    "#             img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n",
    "#             #Convert the rgb values of the input image to the range of 0 to 1\n",
    "#             #1.0/255 indicates that we are using a 24-bit RGB color space.\n",
    "#             #It means that we are using numbers between 0â€“255 for each color channel\n",
    "#             #img_lab = 1.0/225*img_lab\n",
    "#             # resize the lightness channel to network input size \n",
    "#             img_lab_rs = cv2.resize(img_lab, (WIDTH, HEIGHT)) # resize image to network input size\n",
    "#             img_l = img_lab_rs[:,:,0] # pull out L channel\n",
    "#             #img_l -= 50 # subtract 50 for mean-centering\n",
    "#             img_ab = img_lab_rs[:,:,1:]#Extracting the ab channel\n",
    "#             img_ab = img_ab/128\n",
    "#             #The true color values range between -128 and 128. This is the default interval \n",
    "#             #in the Lab color space. By dividing them by 128, they too fall within the -1 to 1 interval.\n",
    "#             X_img.append(img_l)\n",
    "#             y_img.append(img_ab)\n",
    "#         except:\n",
    "#             pass\n",
    "#     X_img = np.array(X_img)\n",
    "#     y_img = np.array(y_img)\n",
    "    \n",
    "#     return X_img,y_img\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
