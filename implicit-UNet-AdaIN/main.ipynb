{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6CH_vkV8IEs",
        "outputId": "ccc64d9e-60d8-4718-b4f5-0a4a870b01ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: mps\n"
          ]
        }
      ],
      "source": [
        "import dataset\n",
        "from visualize import *\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "from skimage import color\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import lpips\n",
        "import torch.nn.init as init\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
        "                else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([4, 128])\n"
          ]
        }
      ],
      "source": [
        "class Adain_Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_dim):\n",
        "        super(Adain_Encoder, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, out_dim, kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = torch.relu(self.conv4(x))\n",
        "        x = torch.relu(self.conv5(x))\n",
        "        \n",
        "        # Global average pooling\n",
        "        x = self.global_avg_pool(x)        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# Define input and output dimensions\n",
        "in_channels = 3  # number of input channels\n",
        "out_dim = 128  # size of the output representation\n",
        "\n",
        "# Create an instance of the ConvNet\n",
        "model = Adain_Encoder(in_channels, out_dim)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(4, in_channels, 256, 256)\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([4, 256, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class AdaIN(nn.Module):\n",
        "    \n",
        "    def __init__(self, style_dim, channels):\n",
        "        super(AdaIN, self).__init__()\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
        "        self.style_scale_transform = nn.Linear(style_dim, channels)\n",
        "        self.style_shift_transform = nn.Linear(style_dim, channels)\n",
        "\n",
        "        ## to ensure they learn different stuff | How tho?\n",
        "        init.normal_(self.style_scale_transform.weight, mean=1.0, std=0.02)\n",
        "        init.normal_(self.style_shift_transform.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        self.style_scale_transform.bias.data.fill_(1)  # Initialize scale to 1\n",
        "        self.style_shift_transform.bias.data.fill_(0)  # Initialize shift to 0\n",
        "\n",
        "    def forward(self, x, style):\n",
        "        '''\n",
        "        x - feature maps from the unet\n",
        "        y - learned (jointly) from encoder\n",
        "\n",
        "        return:\n",
        "        same size as x\n",
        "        '''\n",
        "        # Normalize the input feature map\n",
        "        normalized = self.instance_norm(x)\n",
        "        \n",
        "        # Extract style scale and shift parameters from the style vector\n",
        "        scale = self.style_scale_transform(style)[:, :, None, None]\n",
        "        shift = self.style_shift_transform(style)[:, :, None, None]\n",
        "        \n",
        "        # Apply scale and shift to the normalized feature map\n",
        "        transformed = scale * normalized + shift\n",
        "        \n",
        "        return transformed\n",
        "\n",
        "# Example usage:\n",
        "# Define the dimensions\n",
        "style_dim = 128  # dimensionality of the style vector\n",
        "channels = 256    # number of channels in the feature map\n",
        "\n",
        "# Create an instance of AdaIN\n",
        "adain = AdaIN(style_dim, channels)\n",
        "\n",
        "# Generate random input feature map and style vector\n",
        "x = torch.randn(4, channels, 64, 64)  # Example input feature map\n",
        "style = torch.randn(1, style_dim)     # Example style vector\n",
        "\n",
        "# Apply AdaIN\n",
        "\n",
        "output = adain(x, style)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /Users/matteom/miniconda3/envs/torch/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ],
      "source": [
        "mse_loss_fn = nn.MSELoss()\n",
        "perceptual_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "def perceptual_and_MSE_loss(reproduced_image, original_image): \n",
        "    perceptual_loss = perceptual_loss_fn(reproduced_image, original_image).mean()\n",
        "    mse_loss = mse_loss_fn(reproduced_image, original_image)\n",
        "    return perceptual_loss + mse_loss\n",
        "\n",
        "def plot_four(colored,grayscale,style,output):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot first image\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.imshow(colored)\n",
        "    plt.title('Ground truth - colored')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot second image\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.imshow(grayscale,cmap='gray')\n",
        "    plt.title('Input - grayscale')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot third image\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.imshow(style)\n",
        "    plt.title('Input - style')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot third image\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.imshow(output)\n",
        "    plt.title('Ouptut - colorized')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_histograms(latent_space_values):\n",
        "    # Limit the number of plots to 10 if the list is longer\n",
        "    num_plots = min(len(latent_space_values), 10)\n",
        "    # Calculate the number of rows and columns for the grid\n",
        "    num_rows = (num_plots + 4) // 5  # Ensure at least 1 row\n",
        "    num_cols = min(num_plots, 5)\n",
        "    # Create a figure and axis objects\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 3 * num_rows))\n",
        "    # Flatten the axes if necessary\n",
        "    if num_rows == 1 and num_cols == 1:\n",
        "        axes = np.array([[axes]])\n",
        "    # Plot histograms\n",
        "    for i in range(num_plots):\n",
        "        row_index = i // num_cols\n",
        "        col_index = i % num_cols\n",
        "        if num_rows == 1:\n",
        "            ax = axes[col_index]\n",
        "        else:\n",
        "            ax = axes[row_index, col_index]\n",
        "        ax.hist(latent_space_values[i][\"image\"].detach().cpu().numpy().flatten(), bins=50)\n",
        "        ax.set_title(f'Array {i+1}')\n",
        "    # Hide empty subplots\n",
        "    for i in range(num_plots, num_rows * num_cols):\n",
        "        row_index = i // num_cols\n",
        "        col_index = i % num_cols\n",
        "        axes[row_index, col_index].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xq2e0mN_8IEw"
      },
      "outputs": [],
      "source": [
        "class UNetAdaiN(nn.Module):\n",
        "    def __init__(self, colorspace=\"RGB\", adain_latent_dim=32, dropout_rate=None, verbose=False):\n",
        "        self.colorspace = colorspace\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.verbose = verbose\n",
        "        super(UNetAdaiN, self).__init__()\n",
        "        if self.colorspace == 'RGB':\n",
        "            in_C = 1\n",
        "            out_C = 3\n",
        "        elif self.colorspace == 'LAB':\n",
        "            in_C = 1\n",
        "            out_C = 2\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_C, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "        )\n",
        "        self.maxpool_1to2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "        )\n",
        "        self.maxpool_2to3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.maxpool_3to4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.conv4_2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.maxpool_4to5 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.conv5_2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.conv_transpose_5to6 = nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_5to6 = nn.Conv2d(256, 128, 1)\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv_transpose_6to7 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_6to7 = nn.Conv2d(128, 64, 1)\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "        )\n",
        "        self.conv_transpose_7to8 = nn.ConvTranspose2d(32, 32, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_7to8 = nn.Conv2d(64, 32, 1)\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv2d(32, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "        )\n",
        "        self.conv_transpose8to9 = nn.ConvTranspose2d(16, 16, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_8to9 = nn.Conv2d(32, 16, 1)\n",
        "        self.conv9 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding=1),      # Simmetry broken here: keeps being 64 (from paper)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "        )\n",
        "        self.conv10 = nn.Conv2d(16, out_C, 1)\n",
        "\n",
        "        '''\n",
        "        Conv1 shape torch.Size([16, 64, 128, 128])\n",
        "        Conv2 shape torch.Size([16, 128, 64, 64])\n",
        "        Conv3 shape torch.Size([16, 256, 32, 32])\n",
        "        Conv4 shape torch.Size([16, 512, 16, 16])\n",
        "        Conv5 shape torch.Size([16, 512, 8, 8])\n",
        "        '''\n",
        "\n",
        "        self.encoder_adain1 = AdaIN(adain_latent_dim, 16)\n",
        "        self.encoder_adain2 = AdaIN(adain_latent_dim, 32)\n",
        "        self.encoder_adain3 = AdaIN(adain_latent_dim, 64)\n",
        "        self.encoder_adain4 = AdaIN(adain_latent_dim, 128)\n",
        "        self.encoder_adain5 = AdaIN(adain_latent_dim, 128)\n",
        "\n",
        "        self.decoder_adain4 = AdaIN(adain_latent_dim, 128)\n",
        "        self.decoder_adain3 = AdaIN(adain_latent_dim, 64)\n",
        "        self.decoder_adain2 = AdaIN(adain_latent_dim, 32)\n",
        "        self.decoder_adain1 = AdaIN(adain_latent_dim, 16)\n",
        "\n",
        "        self.style_encoder = Adain_Encoder(in_channels=out_C, out_dim=adain_latent_dim)\n",
        "       \n",
        "\n",
        "    def forward(self,x,style_image):\n",
        "        x=x.to(device)\n",
        "        print(f\"Input shape: {x.shape}\") if self.verbose==True else None\n",
        "        style_image = style_image.to(device)\n",
        "        print(f\"Style image shape: {style_image.shape}\") if self.verbose==True else None\n",
        "        style = self.style_encoder(style_image).to(device)\n",
        "        print(f\"Style embedding shape: {style.shape}\") if self.verbose==True else None\n",
        "\n",
        "        # Encoder\n",
        "        conv1 = self.conv1(x)\n",
        "        conv1 = nn.Dropout2d(p=self.dropout_rate)(conv1) if self.dropout_rate is not None else None\n",
        "        conv1 = self.encoder_adain1(x,style) ## AdaIN\n",
        "        print(f\"Conv1 shape {conv1.shape}\") if self.verbose==True else None\n",
        "\n",
        "        maxpooled_1to2 = self.maxpool_1to2(conv1)\n",
        "        conv2 = self.conv2(maxpooled_1to2)\n",
        "        conv2 = nn.Dropout2d(p=self.dropout_rate)(conv2) if self.dropout_rate is not None else None\n",
        "        conv2 = self.encoder_adain2(conv2,style) ## AdaIN\n",
        "        print(f\"Conv2 shape {conv2.shape}\") if self.verbose==True else None\n",
        "\n",
        "        maxpooled_2to3 = self.maxpool_2to3(conv2)\n",
        "        conv3 = self.conv3(maxpooled_2to3)\n",
        "        conv3 = nn.Dropout2d(p=self.dropout_rate)(conv3) if self.dropout_rate is not None else None\n",
        "        conv3 = self.encoder_adain3(conv3,style) ## AdaIN\n",
        "        print(f\"Conv3 shape {conv3.shape}\") if self.verbose==True else None\n",
        "\n",
        "        maxpooled_3to4 = self.maxpool_3to4(conv3)\n",
        "        conv4_1 = self.conv4(maxpooled_3to4)\n",
        "        conv4_1 = nn.Dropout2d(p=self.dropout_rate)(conv4_1) if self.dropout_rate is not None else None\n",
        "        conv4_1 = self.encoder_adain4(conv4_1,style) ## AdaIN\n",
        "        print(f\"Conv4_1 shape {conv4_1.shape}\") if self.verbose==True else None\n",
        "        conv4_2 = self.conv4_2(conv4_1)\n",
        "        conv4_2 = nn.Dropout2d(p=self.dropout_rate)(conv4_2) if self.dropout_rate is not None else None\n",
        "        conv4_2 = self.encoder_adain4(conv4_2,style) ## AdaIN\n",
        "        print(f\"Conv4_2 shape {conv4_2.shape}\") if self.verbose==True else None\n",
        "\n",
        "        maxpooled_4to5 = self.maxpool_4to5(conv4_2)\n",
        "        conv5_1 = self.conv5(maxpooled_4to5)\n",
        "        conv5_1 = nn.Dropout2d(p=self.dropout_rate)(conv5_1) if self.dropout_rate is not None else None\n",
        "        conv5_1 = self.encoder_adain5(conv5_1,style) ## AdaIN\n",
        "        print(f\"Conv5_1 shape {conv5_1.shape}\") if self.verbose==True else None\n",
        "        conv5_2 = self.conv5_2(conv5_1)\n",
        "        conv5_2 = nn.Dropout2d(p=self.dropout_rate)(conv5_2) if self.dropout_rate is not None else None\n",
        "        conv5_2 = self.encoder_adain5(conv5_2,style) ## AdaIN\n",
        "        print(f\"Conv5_2 shape {conv5_2.shape}\") if self.verbose==True else None\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        conv_transpose6 = self.conv_transpose_5to6(conv5_2)\n",
        "        conv_transpose6 = self.decoder_adain4(conv_transpose6,style)\n",
        "        print(f\"Conv_transpose6 shape {conv_transpose6.shape}, concatenates to conv4_2\") if self.verbose==True else None\n",
        "        concatenation_5to6 = torch.cat((conv4_2,conv_transpose6),1)\n",
        "        skip_fusion_5to6 = self.conv1d_fusing_5to6(concatenation_5to6)\n",
        "        conv6 = self.conv6(skip_fusion_5to6)\n",
        "        conv6 = nn.Dropout2d(p=self.dropout_rate)(conv6) if self.dropout_rate is not None else None\n",
        "        print(f\"Conv6 shape {conv6.shape}\") if self.verbose==True else None\n",
        "\n",
        "        conv_transpose7 = self.conv_transpose_6to7(conv6)\n",
        "        conv_transpose7 = self.decoder_adain3(conv_transpose7,style)\n",
        "        print(f\"Conv_transpose7 shape {conv_transpose7.shape}, concatenates to conv3\") if self.verbose==True else None\n",
        "        concatenation_6to7 = torch.cat((conv3, conv_transpose7),1)\n",
        "        skip_fusion_6to7 = self.conv1d_fusing_6to7(concatenation_6to7)\n",
        "        conv7 = self.conv7(skip_fusion_6to7)\n",
        "        conv7 = nn.Dropout2d(p=self.dropout_rate)(conv7) if self.dropout_rate is not None else None\n",
        "        print(f\"Conv7 shape {conv7.shape}\") if self.verbose==True else None\n",
        "\n",
        "        conv_transpose8 = self.conv_transpose_7to8(conv7)\n",
        "        conv_transpose8 = self.decoder_adain2(conv_transpose8,style)\n",
        "        print(f\"Conv_transpose8 shape {conv_transpose8.shape}, concatenates to conv2\") if self.verbose==True else None\n",
        "        concatenation_7to8 = torch.cat((conv2, conv_transpose8),1)\n",
        "        skip_fusion_7to8 = self.conv1d_fusing_7to8(concatenation_7to8)\n",
        "        conv8 = self.conv8(skip_fusion_7to8)\n",
        "        conv8 = nn.Dropout2d(p=self.dropout_rate)(conv8) if self.dropout_rate is not None else None\n",
        "        print(f\"Conv8 shape {conv8.shape}\") if self.verbose==True else None\n",
        "\n",
        "        conv_transpose9 = self.conv_transpose8to9(conv8)\n",
        "        conv_transpose9 = self.decoder_adain1(conv_transpose9,style)\n",
        "        print(f\"Conv_transpose9 shape {conv_transpose9.shape}, concatenates to conv1\") if self.verbose==True else None\n",
        "        concatenation_8_to9 = torch.cat((conv1, conv_transpose9),1)\n",
        "        skip_fusion_8to9 = self.conv1d_fusing_8to9(concatenation_8_to9)\n",
        "        conv9 = self.conv9(skip_fusion_8to9)\n",
        "        conv9 = nn.Dropout2d(p=self.dropout_rate)(conv9) if self.dropout_rate is not None else None\n",
        "        print(f\"Conv9 shape {conv9.shape}\") if self.verbose==True else None\n",
        "\n",
        "        output = self.conv10(conv9)\n",
        "        print(f\"Output shape {output.shape}\") if self.verbose==True else None\n",
        "\n",
        "        if self.colorspace == 'LAB':\n",
        "            output = torch.cat((x,output),1)\n",
        "            print(f\"Output shape after concatenation of L channel {output.shape}\") if self.verbose==True else None\n",
        "\n",
        "        return style, output\n",
        "\n",
        "    \n",
        "\n",
        "    def train_model(self, train_loader, val_loader,\n",
        "                    epochs=54, \n",
        "                    lr=0.0001, \n",
        "                    optimizer=torch.optim.Adam, \n",
        "                    save_path = \"./model_storage/\", \n",
        "                    save_name_prefix='/',\n",
        "                    colorspace='RGB',\n",
        "                    val_check_every=3,\n",
        "                    plot_every=12,\n",
        "                    plotting_samples=None,\n",
        "                    latent_space_test_images=None):\n",
        "\n",
        "        self.to(device)\n",
        "        self.train()\n",
        "        optimizer = optimizer(self.parameters(), lr=lr)\n",
        "\n",
        "        #for saving progress\n",
        "        best_val_loss = 99999\n",
        "        loss_archive = {\"training\": [], \"validation\":[]}\n",
        "\n",
        "        mb = master_bar(range(epochs))\n",
        "        for epoch in mb:\n",
        "            training_loss = 0\n",
        "            for i, batch_data in progress_bar(enumerate(train_loader), total=len(train_loader), parent=mb):\n",
        "                \n",
        "                # Input grayscale and color image\n",
        "                grayscale_images = batch_data['grayscale_image'].to(device)\n",
        "                if colorspace == 'RGB':\n",
        "                    colour_images = batch_data['image'].to(device)\n",
        "                elif colorspace == 'LAB':\n",
        "                    colour_images = batch_data['image'][:,1:,:,:].to(device)\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                _, reproduced_images = self(grayscale_images,colour_images)\n",
        "                reproduced_images = reproduced_images.to(device)\n",
        "                # Loss\n",
        "                loss = perceptual_and_MSE_loss(reproduced_images, batch_data['image'].to(device))\n",
        "               \n",
        "                training_loss += loss.item()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            \n",
        "            # Average training loss and append to achive\n",
        "            training_loss /= len(train_loader)\n",
        "            loss_archive[\"training\"].append(training_loss)            \n",
        "            with torch.no_grad():\n",
        "                if (epoch)%val_check_every == 0 or (epoch + 1) == epochs:\n",
        "                    valdiation_loss = 0\n",
        "\n",
        "                    for val_data in val_loader:\n",
        "                        # Input grayscale and color image\n",
        "                        grayscale_images = val_data['grayscale_image'].to(device)\n",
        "                        if colorspace == 'RGB':\n",
        "                            colour_images = val_data['image'].to(device)\n",
        "                        elif colorspace == 'LAB':\n",
        "                            colour_images = val_data['image'][:,1:,:,:].to(device)\n",
        "                            \n",
        "                        # Forward pass\n",
        "                        _, reproduced_image = self(grayscale_images,colour_images)\n",
        "                        reproduced_images = reproduced_images.to(device)\n",
        "                       \n",
        "                        loss = perceptual_and_MSE_loss(reproduced_image, val_data['image'].to(device))\n",
        "                        \n",
        "                        valdiation_loss += loss.item()\n",
        "                        \n",
        "                    valdiation_loss /= len(val_loader)\n",
        "                    loss_archive[\"validation\"].append(valdiation_loss)                \n",
        "\n",
        "\n",
        "                    if valdiation_loss < best_val_loss:  # Update best validation loss and save checkpoint if best model\n",
        "                        state_diction = self.state_dict()\n",
        "                        best_val_loss = valdiation_loss\n",
        "                        for key in state_diction.keys():\n",
        "                            state_diction[key] = state_diction[key].to(torch.device('cpu'))\n",
        "                        torch.save(state_diction, (save_path+save_name_prefix+f\"_best_model.pth.tar\"))\n",
        "\n",
        "                    #construct df with losses\n",
        "                    loss_df = pd.DataFrame({'epoch': range(0,epoch+1),  \n",
        "                                            'training_loss':loss_archive['training'], \n",
        "                                            'validation_loss': loss_archive['validation']})\n",
        "                    loss_df.to_csv(save_path+save_name_prefix+\"_loss.csv\", index=False)\n",
        "\n",
        "                    # Plot losses\n",
        "                    plt.clf()\n",
        "                    plt.plot(loss_df['epoch'], loss_df['training_loss'], label='Training Loss')\n",
        "                    plt.plot(loss_df['epoch'], loss_df['validation_loss'].interpolate(method='linear'), label='Validation Loss')\n",
        "                    plt.xlabel('Epoch')\n",
        "                    plt.ylabel('Loss')\n",
        "                    plt.title('Training and Validation Loss')\n",
        "                    plt.legend()\n",
        "                    plt.savefig(save_path+save_name_prefix+'_loss_plot.png')\n",
        "                    plt.clf()\n",
        "\n",
        "                    # Plot images three triplets of images\n",
        "                    if (epoch)%plot_every==0 and plotting_samples is not None:\n",
        "                        if colorspace == 'RGB':\n",
        "                            # 0\n",
        "                            ground_truth = plotting_samples[0]['image']\n",
        "                            grayscale_image = plotting_samples[0]['grayscale_image']\n",
        "                            _,recreated_image = self(grayscale_image.unsqueeze(0).to(device),\n",
        "                                                ground_truth.unsqueeze(0).to(device))\n",
        "                            recreated_image = recreated_image.detach().cpu().squeeze(0)\n",
        "                            plot_four(colored=ground_truth.permute(1,2,0).numpy(),\n",
        "                                    grayscale=grayscale_image.numpy().squeeze(0),\n",
        "                                    style=ground_truth.permute(1,2,0).numpy(),\n",
        "                                    output=recreated_image.permute(1,2,0).numpy())\n",
        "                            # 1\n",
        "                            ground_truth = plotting_samples[1]['image']\n",
        "                            grayscale_image = plotting_samples[1]['grayscale_image']\n",
        "                            _,recreated_image = self(grayscale_image.unsqueeze(0).to(device),\n",
        "                                                ground_truth.unsqueeze(0).to(device))\n",
        "                            recreated_image = recreated_image.detach().cpu().squeeze(0)\n",
        "                            plot_four(colored=ground_truth.permute(1,2,0).numpy(),\n",
        "                                    grayscale=grayscale_image.numpy().squeeze(0),\n",
        "                                    style=ground_truth.permute(1,2,0).numpy(),\n",
        "                                    output=recreated_image.permute(1,2,0).numpy())\n",
        "                        elif colorspace == 'LAB':\n",
        "                            # 0\n",
        "                            ground_truth = plotting_samples[0]['image']\n",
        "                            grayscale_image = plotting_samples[0]['grayscale_image']\n",
        "                            input_encoder = ground_truth[1:,:,:].unsqueeze(0).to(device)\n",
        "                            lab2rgb = dataset.LABtoRGB()\n",
        "                            print(f\"ground_truth shape {ground_truth.shape}\")\n",
        "                            original_color = lab2rgb(ground_truth.to(device))\n",
        "\n",
        "                            _,recreated_image = self(grayscale_image.unsqueeze(0).to(device),\n",
        "                                                    input_encoder)\n",
        "                            \n",
        "                            recreated_image = lab2rgb(recreated_image.detach().cpu().squeeze(0))\n",
        "                            print(f\"recreated_image shape: {recreated_image.shape}\")\n",
        "                            plot_four(colored=original_color.permute(1,2,0).numpy(),\n",
        "                                    grayscale=grayscale_image.numpy().squeeze(0),\n",
        "                                    style=original_color.permute(1,2,0).numpy(),\n",
        "                                    output=recreated_image.permute(1,2,0).numpy())\n",
        "                            # 1\n",
        "                            ground_truth = plotting_samples[1]['image']\n",
        "                            grayscale_image = plotting_samples[1]['grayscale_image']\n",
        "                            input_encoder = ground_truth[1:,:,:].unsqueeze(0).to(device)\n",
        "                            lab2rgb = dataset.LABtoRGB()\n",
        "                            print(f\"ground_truth shape {ground_truth.shape}\")\n",
        "                            original_color = lab2rgb(ground_truth.to(device))\n",
        "\n",
        "                            _,recreated_image = self(grayscale_image.unsqueeze(0).to(device),\n",
        "                                                    input_encoder)\n",
        "                            \n",
        "                            recreated_image = lab2rgb(recreated_image.detach().cpu().squeeze(0))\n",
        "                            print(f\"recreated_image shape: {recreated_image.shape}\")\n",
        "                            plot_four(colored=original_color.permute(1,2,0).numpy(),\n",
        "                                    grayscale=grayscale_image.numpy().squeeze(0),\n",
        "                                    style=original_color.permute(1,2,0).numpy(),\n",
        "                                    output=recreated_image.permute(1,2,0).numpy())\n",
        "                        \n",
        "                    if (epoch)%plot_every==0 and latent_space_test_images is not None:    \n",
        "                        plot_histograms(latent_space_values=list(latent_space_test_images))\n",
        "                        \n",
        "                else:\n",
        "                    loss_archive[\"validation\"].append(np.nan)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "RESOLUTION = (128,128)\n",
        "COLORSPACE = 'LAB'\n",
        "TRAIN_SIZE = 100000\n",
        "VAL_SIZE = 1000\n",
        "ADAIN_LATENT_SPACE_DIM = 32\n",
        "\n",
        "train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace=COLORSPACE,resolution=RESOLUTION)\n",
        "train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_from_checkpoint(checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model = UNetAdaiN(colorspace=COLORSPACE,adain_latent_dim=ADAIN_LATENT_SPACE_DIM, dropout_rate=0.1, verbose=False)\n",
        "        model.load_state_dict(checkpoint)\n",
        "        print(\"Model loaded successfully from checkpoint.\")\n",
        "        return model.to(device)\n",
        "    else:\n",
        "        print(\"Checkpoint file does not exist.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Device is: \", device)\n",
        "\n",
        "model = load_model_from_checkpoint(\"./implicit_scarsity_experiments/RGB_Latent32_best_model.pth.tar\")\n",
        "#model = UNetAdaiN(colorspace=COLORSPACE, adain_latent_dim=ADAIN_LATENT_SPACE_DIM, dropout_rate=0.1, verbose=False)\n",
        "model = model.to(device)\n",
        "\n",
        "model.train_model(train_loader=train_loader, val_loader=validation_loader, \n",
        "                        epochs=10, lr=0.00001, optimizer=torch.optim.Adam,\n",
        "                        save_path= 'implicit_scarsity_experiments/', \n",
        "                        save_name_prefix='/RGB_Latent32_trsize100k_valsize1k',\n",
        "                        colorspace=COLORSPACE, val_check_every=1, plot_every=1, \n",
        "                        plotting_samples=[list(train_data)[2], list(validation_data)[7]],\n",
        "                        latent_space_test_images=list(validation_data)[:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing style transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s_train_data, s_validation_data = dataset.prepare_styles_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace=COLORSPACE,resolution=RESOLUTION)\n",
        "_, s_validation_loader = dataset.prepare_styles_dataloader(s_train_data, s_validation_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only to choose painings\n",
        "list_paintings =list(s_validation_data)\n",
        "if COLORSPACE =='LAB':\n",
        "    lab2rgb = dataset.LABtoRGB()\n",
        "    list_paintings = [{'image':lab2rgb(image['image'])} for image in list_paintings]\n",
        "plot_four(list_paintings[0]['image'].permute(1,2,0).numpy(),\n",
        "          list_paintings[1]['image'].permute(1,2,0).numpy(),\n",
        "          list_paintings[2]['image'].permute(1,2,0).numpy(),\n",
        "          list_paintings[3]['image'].permute(1,2,0).numpy())\n",
        "plot_four(list_paintings[4]['image'].permute(1,2,0).numpy(),\n",
        "          list_paintings[5]['image'].permute(1,2,0).numpy(),\n",
        "          list_paintings[6]['image'].permute(1,2,0).numpy(),\n",
        "          list_paintings[7]['image'].permute(1,2,0).numpy())\n",
        "plot_four(list_paintings[8]['image'].permute(1,2,0).numpy(),\n",
        "            list_paintings[9]['image'].permute(1,2,0).numpy(),\n",
        "            list_paintings[10]['image'].permute(1,2,0).numpy(),\n",
        "            list_paintings[11]['image'].permute(1,2,0).numpy())\n",
        "plot_four(list_paintings[12]['image'].permute(1,2,0).numpy(),\n",
        "            list_paintings[13]['image'].permute(1,2,0).numpy(),\n",
        "            list_paintings[14]['image'].permute(1,2,0).numpy(),\n",
        "            list_paintings[15]['image'].permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test colorization against different style images\n",
        "\n",
        "model = load_model_from_checkpoint(\"./implicit_scarsity_experiments/LAB_Latent32_trsize50000_valsize500_best_model.pth.tar\")\n",
        "model=model.to(device)\n",
        "plotting_sample=list(validation_data)[7]\n",
        "\n",
        "style_image = list(s_validation_data)[10]['image']\n",
        "\n",
        "if COLORSPACE == 'RGB':\n",
        "        # 0\n",
        "        ground_truth = plotting_sample['image']\n",
        "        grayscale_image = plotting_sample['grayscale_image']\n",
        "        _,recreated_image = model(grayscale_image.unsqueeze(0).to(device),\n",
        "                        ground_truth.unsqueeze(0).to(device))\n",
        "        recreated_image = recreated_image.detach().cpu().squeeze(0)\n",
        "        plot_four(colored=ground_truth.permute(1,2,0).numpy(),\n",
        "                grayscale=grayscale_image.numpy().squeeze(0),\n",
        "                style=ground_truth.permute(1,2,0).numpy(),\n",
        "                output=recreated_image.permute(1,2,0).numpy())\n",
        "        # 1\n",
        "        ground_truth = plotting_sample['image']\n",
        "        grayscale_image = plotting_sample['grayscale_image']\n",
        "        _,recreated_image = model(grayscale_image.unsqueeze(0).to(device),\n",
        "                        style_image.unsqueeze(0).to(device))\n",
        "        recreated_image = recreated_image.detach().cpu().squeeze(0)\n",
        "        plot_four(colored=ground_truth.permute(1,2,0).numpy(),\n",
        "                grayscale=grayscale_image.numpy().squeeze(0),\n",
        "                style=style_image.permute(1,2,0).numpy(),\n",
        "                output=recreated_image.permute(1,2,0).numpy())\n",
        "elif COLORSPACE == 'LAB':\n",
        "        # 0\n",
        "        ground_truth = plotting_sample['image']\n",
        "        grayscale_image = plotting_sample['grayscale_image']\n",
        "        input_encoder = ground_truth[1:,:,:].unsqueeze(0).to(device)\n",
        "        lab2rgb = dataset.LABtoRGB()\n",
        "        original_color = lab2rgb(ground_truth.to(device))\n",
        "\n",
        "        _,recreated_image = model(grayscale_image.unsqueeze(0).to(device),\n",
        "                                input_encoder)\n",
        "\n",
        "        recreated_image = lab2rgb(recreated_image.detach().cpu().squeeze(0))\n",
        "        plot_four(colored=original_color.permute(1,2,0).numpy(),\n",
        "                grayscale=grayscale_image.numpy().squeeze(0),\n",
        "                style=original_color.permute(1,2,0).numpy(),\n",
        "                output=recreated_image.permute(1,2,0).numpy())\n",
        "        # 1\n",
        "        ground_truth = plotting_sample['image']\n",
        "        grayscale_image = plotting_sample['grayscale_image']\n",
        "        input_encoder = style_image[1:,:,:].unsqueeze(0).to(device)\n",
        "        lab2rgb = dataset.LABtoRGB()\n",
        "        \n",
        "        original_color = lab2rgb(ground_truth.to(device))\n",
        "\n",
        "        _,recreated_image = model(grayscale_image.unsqueeze(0).to(device),\n",
        "                                input_encoder)\n",
        "\n",
        "        recreated_image = lab2rgb(recreated_image.detach().cpu().squeeze(0))\n",
        "        style_image = lab2rgb(style_image)\n",
        "        plot_four(colored=original_color.permute(1,2,0).numpy(),\n",
        "                grayscale=grayscale_image.numpy().squeeze(0),\n",
        "                style=style_image.permute(1,2,0).numpy(),\n",
        "                output=recreated_image.permute(1,2,0).numpy())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
