{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6CH_vkV8IEs",
        "outputId": "ccc64d9e-60d8-4718-b4f5-0a4a870b01ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: mps\n"
          ]
        }
      ],
      "source": [
        "import dataset\n",
        "from visualize import *\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "from skimage import color\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
        "                else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /Users/matteom/miniconda3/envs/torch/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ],
      "source": [
        "import lpips\n",
        "perceptual_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "# d = loss_fn.forward(im0,im1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([4, 128])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class Adain_Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_dim):\n",
        "        super(Adain_Encoder, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, out_dim, kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = torch.relu(self.conv4(x))\n",
        "        x = torch.relu(self.conv5(x))\n",
        "        \n",
        "        # Global average pooling\n",
        "        x = self.global_avg_pool(x)        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# Define input and output dimensions\n",
        "in_channels = 3  # number of input channels\n",
        "out_dim = 128  # size of the output representation\n",
        "\n",
        "# Create an instance of the ConvNet\n",
        "model = Adain_Encoder(in_channels, out_dim)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(4, in_channels, 256, 256)\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([4, 256, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    \n",
        "    def __init__(self, style_dim, channels):\n",
        "        super(AdaIN, self).__init__()\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
        "        self.style_scale_transform = nn.Linear(style_dim, channels)\n",
        "        self.style_shift_transform = nn.Linear(style_dim, channels)\n",
        "\n",
        "        ## to ensure they learn different stuff | How tho?\n",
        "        init.normal_(self.style_scale_transform.weight, mean=1.0, std=0.02)\n",
        "        init.normal_(self.style_shift_transform.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        self.style_scale_transform.bias.data.fill_(1)  # Initialize scale to 1\n",
        "        self.style_shift_transform.bias.data.fill_(0)  # Initialize shift to 0\n",
        "\n",
        "    def forward(self, x, style):\n",
        "        '''\n",
        "        x - feature maps from the unet\n",
        "        y - learned (jointly) from encoder\n",
        "\n",
        "        return:\n",
        "        same size as x\n",
        "        '''\n",
        "        # Normalize the input feature map\n",
        "        normalized = self.instance_norm(x)\n",
        "        \n",
        "        # Extract style scale and shift parameters from the style vector\n",
        "        scale = self.style_scale_transform(style)[:, :, None, None]\n",
        "        shift = self.style_shift_transform(style)[:, :, None, None]\n",
        "        \n",
        "        # Apply scale and shift to the normalized feature map\n",
        "        transformed = scale * normalized + shift\n",
        "        \n",
        "        return transformed\n",
        "\n",
        "# Example usage:\n",
        "# Define the dimensions\n",
        "style_dim = 128  # dimensionality of the style vector\n",
        "channels = 256    # number of channels in the feature map\n",
        "\n",
        "# Create an instance of AdaIN\n",
        "adain = AdaIN(style_dim, channels)\n",
        "\n",
        "# Generate random input feature map and style vector\n",
        "x = torch.randn(4, channels, 64, 64)  # Example input feature map\n",
        "style = torch.randn(1, style_dim)     # Example style vector\n",
        "\n",
        "# Apply AdaIN\n",
        "\n",
        "output = adain(x, style)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse_loss_fn = nn.MSELoss()\n",
        "def perceptual_and_MSE_loss(reproduced_image, original_image):\n",
        "    '''\n",
        "    reproduced_image: output of the model\n",
        "    original_image: ground truth\n",
        "    '''\n",
        "    # Perceptual loss\n",
        "    perceptual_loss = perceptual_loss_fn(reproduced_image, original_image).mean()\n",
        "    \n",
        "    # MSE loss\n",
        "    mse_loss = mse_loss_fn(reproduced_image, original_image)\n",
        "    \n",
        "    return perceptual_loss + mse_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xq2e0mN_8IEw"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_C = 3, out_C=3, style_dim=128):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_C, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.maxpool_1to2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.maxpool_2to3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.maxpool_3to4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "        self.maxpool_4to5 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.conv_transpose_5to6 = nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_5to6 = nn.Conv2d(1024, 512, 1)\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.conv_transpose_6to7 = nn.ConvTranspose2d(256, 256, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_6to7 = nn.Conv2d(512, 256, 1)\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.conv_transpose_7to8 = nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_7to8 = nn.Conv2d(256, 128, 1)\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv_transpose8to9 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_8to9 = nn.Conv2d(128, 64, 1)\n",
        "        self.conv9 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, padding=1),      # Simmetry broken here: keeps being 64 (from paper)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv10 = nn.Conv2d(64, out_C, 1)\n",
        "\n",
        "        '''\n",
        "        Conv1 shape torch.Size([16, 64, 128, 128])\n",
        "        Conv2 shape torch.Size([16, 128, 64, 64])\n",
        "        Conv3 shape torch.Size([16, 256, 32, 32])\n",
        "        Conv4 shape torch.Size([16, 512, 16, 16])\n",
        "        Conv5 shape torch.Size([16, 512, 8, 8])\n",
        "        '''\n",
        "\n",
        "        self.encoder_adain1 = AdaIN(128, 64)\n",
        "        self.encoder_adain2 = AdaIN(128, 128)\n",
        "        self.encoder_adain3 = AdaIN(128, 256)\n",
        "        self.encoder_adain4 = AdaIN(128, 512)\n",
        "        self.encoder_adain5 = AdaIN(128, 512)\n",
        "\n",
        "        self.decoder_adain4 = AdaIN(128, 512)\n",
        "        self.decoder_adain3 = AdaIN(128, 256)\n",
        "        self.decoder_adain2 = AdaIN(128, 128)\n",
        "        self.decoder_adain1 = AdaIN(128, 64)\n",
        "\n",
        "        if out_C == 3:\n",
        "            self.style_encoder = Adain_Encoder(in_channels=3, out_dim=128)\n",
        "        elif out_C == 2:\n",
        "            self.style_encoder = Adain_Encoder(in_channels=2, out_dim=128)\n",
        "\n",
        "\n",
        "    def forward(self,x,style_image):\n",
        "        x=x.to(device)\n",
        "        style_image = style_image.to(device)\n",
        "\n",
        "        style = self.style_encoder(style_image).to(device)\n",
        "\n",
        "        # Encoder\n",
        "        conv1 = self.conv1(x)\n",
        "        conv1 = self.encoder_adain1(x,style) ## AdaIN\n",
        "\n",
        "        maxpooled_1to2 = self.maxpool_1to2(conv1)\n",
        "        conv2 = self.conv2(maxpooled_1to2)\n",
        "        conv2 = self.encoder_adain2(conv2,style) ## AdaIN\n",
        "\n",
        "        maxpooled_2to3 = self.maxpool_2to3(conv2)\n",
        "        conv3 = self.conv3(maxpooled_2to3)\n",
        "        conv3 = self.encoder_adain3(conv3,style) ## AdaIN\n",
        "\n",
        "        maxpooled_3to4 = self.maxpool_3to4(conv3)\n",
        "        conv4 = self.conv4(maxpooled_3to4)\n",
        "        conv4 = self.encoder_adain4(conv4,style) ## AdaIN\n",
        "\n",
        "        maxpooled_4to5 = self.maxpool_4to5(conv4)\n",
        "        conv5 = self.conv5(maxpooled_4to5)\n",
        "        conv5 = self.encoder_adain5(conv5,style) ## AdaIN\n",
        "        # Decoder\n",
        "        concatenation_5to6 = torch.cat((conv4,self.decoder_adain4(self.conv_transpose_5to6(conv5),style)),1)\n",
        "        # concatenation_5to6 = torch.cat((conv4,self.conv_transpose_5to6(conv5)),1)\n",
        "        skip_fusion_5to6 = self.conv1d_fusing_5to6(concatenation_5to6)\n",
        "        conv6 = self.conv6(skip_fusion_5to6)\n",
        "\n",
        "        concatenation_6to7 = torch.cat((conv3, self.decoder_adain3(self.conv_transpose_6to7(conv6),style)),1)\n",
        "        # concatenation_6to7 = torch.cat((conv3, self.conv_transpose_6to7(conv6)),1)\n",
        "        skip_fusion_6to7 = self.conv1d_fusing_6to7(concatenation_6to7)\n",
        "        conv7 = self.conv7(skip_fusion_6to7)\n",
        "\n",
        "        concatenation_7to8 = torch.cat((conv2, self.decoder_adain2(self.conv_transpose_7to8(conv7),style)),1)\n",
        "        # concatenation_7to8 = torch.cat((conv2, self.conv_transpose_7to8(conv7)),1)\n",
        "        skip_fusion_7to8 = self.conv1d_fusing_7to8(concatenation_7to8)\n",
        "        conv8 = self.conv8(skip_fusion_7to8)\n",
        "\n",
        "        concatenation_8_to9 = torch.cat((conv1, self.decoder_adain1(self.conv_transpose8to9(conv8),style)),1)\n",
        "        # concatenation_8_to9 = torch.cat((conv1, self.conv_transpose8to9(conv8)),1)\n",
        "        skip_fusion_8to9 = self.conv1d_fusing_8to9(concatenation_8_to9)\n",
        "        conv9 = self.conv9(skip_fusion_8to9)\n",
        "\n",
        "        output = self.conv10(conv9)\n",
        "        return style,output\n",
        "    \n",
        "    def style_loss(self, reproduced_image, encoded_style):\n",
        "        #encoded_style = self.style_encoder(style_image)\n",
        "        with torch.no_grad():\n",
        "            encoded_style_of_reproduced = self.style_encoder(reproduced_image)\n",
        "        return mse_loss_fn(encoded_style, encoded_style_of_reproduced)\n",
        "    \n",
        "    def colorization_plus_style_loss(self, reproduced_image, original_image, encoded_style, alpha=0.5):\n",
        "        colorization_loss = perceptual_and_MSE_loss(reproduced_image, original_image)\n",
        "        style_loss = self.style_loss(reproduced_image, encoded_style)\n",
        "        return alpha * colorization_loss + (1 - alpha) * style_loss, colorization_loss, style_loss\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, style_train_loader, style_val_loader,\n",
        "                    epochs=54, \n",
        "                    lr=0.0001, \n",
        "                    loss_balance=0.5,\n",
        "                    optimizer=torch.optim.Adam, \n",
        "                    verbose=True, \n",
        "                    save_path = \"./model_storage/\", \n",
        "                    save_name_prefix='/',\n",
        "                    colorspace='RGB',\n",
        "                    validate_and_save_every=3):\n",
        "\n",
        "        loss_df = pd.DataFrame(['training_loss', 'validation_loss'])\n",
        "        self.to(device)\n",
        "        self.train()\n",
        "        optimizer_colorisation = optimizer(self.parameters(), lr=lr)\n",
        "        optimizer_style = optimizer(self.parameters(), lr=lr*0.01)\n",
        "\n",
        "        #for saving progress\n",
        "        best_val_loss = 99999\n",
        "        loss_archive = {\"training\": {\"combined\": [],\"colorization\":[],\"style\":[]}, \"validation\": {\"combined\": [],\"colorization\":[],\"style\":[]}}\n",
        "\n",
        "        mb = master_bar(range(epochs))\n",
        "        for epoch in mb:\n",
        "            training_combined_loss = 0\n",
        "            training_colorizaiton_loss = 0\n",
        "            training_style_loss = 0\n",
        "            total=len(train_loader)\n",
        "            for i, batch_data in progress_bar(enumerate(train_loader), total=total, parent=mb):\n",
        "                if i == total-1:\n",
        "                    break\n",
        "\n",
        "                # Input grayscale image\n",
        "                image = batch_data['grayscale_image'].to(device)\n",
        "\n",
        "                # A batch of style iimages\n",
        "                for style_batch_data in style_train_loader:\n",
        "                    batch_of_style_images = style_batch_data['image'].to(device)\n",
        "                    break\n",
        "\n",
        "                # Colorization forward and backpropagation\n",
        "                optimizer_colorisation.zero_grad()\n",
        "                encoded_style, reproduced_image = self(image,batch_of_style_images)\n",
        "                encoded_style = encoded_style.to(device)\n",
        "                reproduced_image = reproduced_image.to(device)\n",
        "                if colorspace == 'RGB':\n",
        "                    combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(reproduced_image, batch_data['image'].to(device), encoded_style, alpha=loss_balance)\n",
        "                elif colorspace == 'LAB':\n",
        "                    combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(reproduced_image, batch_data['image'][:,1:,:,:].to(device), encoded_style, alpha=loss_balance)\n",
        "                #training_combined_loss += combined_loss.item()\n",
        "                training_colorizaiton_loss += colorization_loss.item()\n",
        "                #training_style_loss += style_loss.item()\n",
        "                colorization_loss.backward()    # ----\n",
        "                optimizer_colorisation.step()\n",
        "\n",
        "                # Style forward and backpropagation\n",
        "                optimizer_style.zero_grad()\n",
        "                encoded_style, reproduced_image = self(image,batch_of_style_images)\n",
        "                encoded_style = encoded_style.to(device)\n",
        "                reproduced_image = reproduced_image.to(device)\n",
        "                if colorspace == 'RGB':\n",
        "                    combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(reproduced_image, batch_data['image'].to(device), encoded_style, alpha=loss_balance)\n",
        "                elif colorspace == 'LAB':\n",
        "                    combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(reproduced_image, batch_data['image'][:,1:,:,:].to(device), encoded_style, alpha=loss_balance)\n",
        "                #training_combined_loss += combined_loss.item()\n",
        "                #training_colorizaiton_loss += colorization_loss.item()\n",
        "                training_style_loss += style_loss.item()\n",
        "                style_loss.backward() # ----\n",
        "                optimizer_style.step()\n",
        "            \n",
        "            \n",
        "            #average training loss and append to achive\n",
        "            training_combined_loss /= len(train_loader)\n",
        "            loss_archive[\"training\"][\"combined\"].append(training_combined_loss)\n",
        "            training_colorizaiton_loss /= len(train_loader)\n",
        "            loss_archive[\"training\"][\"colorization\"].append(training_colorizaiton_loss)\n",
        "            training_style_loss /= len(train_loader)\n",
        "            loss_archive[\"training\"][\"style\"].append(training_style_loss)\n",
        "            \n",
        "\n",
        "            if (epoch)%validate_and_save_every == 0 or (epoch + 1) == epochs:\n",
        "                validation_combined_loss = 0\n",
        "                validation_colorizaiton_loss = 0\n",
        "                validation_style_loss = 0\n",
        "                with torch.no_grad():  # Disable gradient calculation for validation\n",
        "                    total=len(val_loader)\n",
        "                    for i,val_data in enumerate(val_loader):\n",
        "                        if i == total-1:\n",
        "                            break\n",
        "                        # Input grayscale image\n",
        "                        val_image = val_data['grayscale_image'].to(device)\n",
        "                        # A batch of style images\n",
        "                        for style_batch_data in style_val_loader:\n",
        "                            val_batch_of_style_images = style_batch_data['image'].to(device)\n",
        "                            break\n",
        "                        \n",
        "                        # Forward pass\n",
        "                        encoded_style, val_reproduced_image = self(image,val_batch_of_style_images)\n",
        "                        encoded_style = encoded_style.to(device)\n",
        "                        val_reproduced_image = val_reproduced_image.to(device)\n",
        "                        if colorspace == 'RGB':\n",
        "                            combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(val_reproduced_image, val_data['image'].to(device),encoded_style,loss_balance)\n",
        "                        elif colorspace == 'LAB':\n",
        "                            combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(val_reproduced_image, val_data['image'][:,1:,:,:].to(device),encoded_style,loss_balance)\n",
        "                        validation_combined_loss += combined_loss.item()\n",
        "                        validation_colorizaiton_loss += colorization_loss.item()\n",
        "                        validation_style_loss += style_loss.item()\n",
        "                       \n",
        "                validation_combined_loss /= len(val_loader)\n",
        "                loss_archive[\"validation\"][\"combined\"].append(validation_combined_loss)\n",
        "                validation_colorizaiton_loss /= len(val_loader)\n",
        "                loss_archive[\"validation\"][\"colorization\"].append(validation_colorizaiton_loss)\n",
        "                validation_style_loss /= len(val_loader)\n",
        "                loss_archive[\"validation\"][\"style\"].append(validation_style_loss)\n",
        "                \n",
        "\n",
        "                if verbose:\n",
        "                  print(f\"Epoch {epoch}, training_loss = {training_combined_loss}\")\n",
        "                  print(f\"Epoch {epoch}: validation loss = {validation_combined_loss}\")\n",
        "\n",
        "\n",
        "                if validation_combined_loss < best_val_loss:  # Update best validation loss and save checkpoint if best model\n",
        "                  state_diction = self.state_dict()\n",
        "                  best_val_loss = validation_combined_loss\n",
        "                  for key in state_diction.keys():\n",
        "                      state_diction[key] = state_diction[key].to(torch.device('cpu'))\n",
        "                  torch.save(state_diction, (save_path+save_name_prefix+f\"_best_model.pth.tar\"))\n",
        "\n",
        "                #construct df with losses\n",
        "                loss_df = pd.DataFrame({'epoch': range(0,epoch+1),  \n",
        "                                        'training_combined_loss':loss_archive['training'][\"combined\"], \n",
        "                                        \"training_colorization_loss\": loss_archive['training'][\"colorization\"], \n",
        "                                        \"training_style_loss\": loss_archive['training'][\"style\"],\n",
        "                                        'validation_combined_loss': loss_archive['validation'][\"combined\"],\n",
        "                                        \"validation_colorization_loss\": loss_archive['validation'][\"colorization\"],\n",
        "                                        \"validation_style_loss\": loss_archive['validation'][\"style\"]})\n",
        "                loss_df.to_csv(save_path+save_name_prefix+\"_loss.csv\", index=False)\n",
        "                \n",
        "                # Plot colorisation losses\n",
        "                plt.plot(loss_df[\"epoch\"], loss_df[\"training_colorization_loss\"], label=\"Training Colorization Loss\",linestyle='dashed')\n",
        "                plt.plot(loss_df[\"epoch\"], loss_df[\"validation_colorization_loss\"].interpolate(method='linear'), label=\"Validation Colorization Loss\",linestyle='dashed')\n",
        "\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.title('Colorization Loss')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.savefig(save_path+save_name_prefix+'_colorization_loss_plot.png')\n",
        "                plt.clf()\n",
        "\n",
        "                # Plot style losses\n",
        "                plt.plot(loss_df[\"epoch\"], loss_df[\"training_style_loss\"], label=\"Training Style Loss\",linestyle='dotted')\n",
        "                plt.plot(loss_df[\"epoch\"], loss_df[\"validation_style_loss\"].interpolate(method='linear'), label=\"Validation Style Loss\",linestyle='dotted')\n",
        "\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.title('Style Loss')\n",
        "                plt.legend()\n",
        "\n",
        "                # Save plot as .png file\n",
        "                plt.savefig(save_path+save_name_prefix+'_style_loss_plot.png')\n",
        "                plt.clf()\n",
        "\n",
        "\n",
        "                \n",
        "            else:\n",
        "                loss_archive[\"validation\"][\"combined\"].append(np.nan)\n",
        "                loss_archive[\"validation\"][\"colorization\"].append(np.nan)\n",
        "                loss_archive[\"validation\"][\"style\"].append(np.nan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_from_checkpoint(checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        # Load the checkpoint\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        \n",
        "        # Create model architecture\n",
        "        # For example, if your model class is named MyModel:\n",
        "        model = UNet(in_C=1, out_C=3)\n",
        "        \n",
        "        # Load state_dict into the model\n",
        "        model.load_state_dict(checkpoint)\n",
        "        \n",
        "        # Optionally, load other elements from the checkpoint such as optimizer state, etc.\n",
        "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        \n",
        "        print(\"Model loaded successfully from checkpoint.\")\n",
        "        return model.to(device)\n",
        "    else:\n",
        "        print(\"Checkpoint file does not exist.\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colorization from RGB color space. \n",
        "\n",
        "Input is a HxW grayscale image.\n",
        "Output is a 3xHxW RGB image.\n",
        "\n",
        "No std and mean normalization is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kept apart as it must be the SAME for images and styles!\n",
        "BATCH_SIZE = 16\n",
        "RESOLUTION = (128,128)\n",
        "COLORSPACE = 'RGB'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully\n",
            "Data loader prepared successfully\n"
          ]
        }
      ],
      "source": [
        "# Actual images:\n",
        "TRAIN_SIZE = 5000\n",
        "VAL_SIZE = 500\n",
        "\n",
        "train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace=COLORSPACE,resolution=RESOLUTION)\n",
        "train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ec78232967d4f639ad26833a34e88d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully\n",
            "Data loader prepared successfully\n"
          ]
        }
      ],
      "source": [
        "# Styles:\n",
        "S_TRAIN_SIZE = 500\n",
        "S_VAL_SIZE = 50\n",
        "\n",
        "style_train_data, style_validation_data = dataset.prepare_styles_dataset(train_size=S_TRAIN_SIZE, test_size=S_VAL_SIZE, batch_size=BATCH_SIZE,colorspace=COLORSPACE,resolution=RESOLUTION)\n",
        "style_train_loader, style_validation_loader = dataset.prepare_styles_dataloader(style_train_data, style_validation_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is:  mps\n",
            "Model loaded successfully from checkpoint.\n"
          ]
        }
      ],
      "source": [
        "print(\"Device is: \", device)\n",
        "model = UNet(in_C=1, out_C=3).to(device)\n",
        "\n",
        "if TRAIN:\n",
        "    model.train_model(train_loader=train_loader, val_loader=validation_loader, \n",
        "                        style_train_loader=style_train_loader, style_val_loader=style_validation_loader,\n",
        "                        epochs=100, lr=0.0001, loss_balance=0.7, optimizer=torch.optim.Adam, verbose=False,\n",
        "                        save_path= 'combined_losses_experiments/', \n",
        "                        save_name_prefix='/UNet-AdaIN_trsize500-100_valsize50-10_combloss07.pth.tar')\n",
        "    state_diction = model.state_dict()\n",
        "\n",
        "    for key in state_diction.keys():\n",
        "        state_diction[key] = state_diction[key].to(torch.device('cpu'))\n",
        "\n",
        "    torch.save(state_diction, \"UNet-AdaIN_trsize500-100_valsize50-10_combined_losses.pth.tar\")\n",
        "else:\n",
        "    model = load_model_from_checkpoint(\"/Users/matteom/shared-folder/artistic-colorization-UNet/Unet_adain/combined_losses_experiments/UNet-AdaIN_trsize5000_valsize500_best_model.pth.tar\")\n",
        "    model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is:  mps\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='7' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      46.67% [7/15 3:18:18&lt;3:46:38]\n",
              "    </div>\n",
              "    \n",
              "\n",
              "\n",
              "    <div>\n",
              "      <progress value='98' class='' max='309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      31.72% [98/309 08:36&lt;18:32]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Device is: \", device)\n",
        "model = UNet(in_C=1, out_C=3).to(device)\n",
        "\n",
        "if TRAIN:\n",
        "    model.train_model(train_loader=train_loader, val_loader=validation_loader, \n",
        "                        style_train_loader=style_train_loader, style_val_loader=style_validation_loader,\n",
        "                        epochs=15, lr=0.0001, loss_balance=0.7, optimizer=torch.optim.Adam, verbose=False,\n",
        "                        save_path= 'dual-opt_experiments/', \n",
        "                        save_name_prefix='/UNet-AdaIN_trsize5000-500_valsize500-50_dual-opt.pth.tar',\n",
        "                        validate_and_save_every=1)\n",
        "    state_diction = model.state_dict()\n",
        "\n",
        "    for key in state_diction.keys():\n",
        "        state_diction[key] = state_diction[key].to(torch.device('cpu'))\n",
        "\n",
        "    torch.save(state_diction, \"UNet-AdaIN_trsize500-100_valsize50-10_combined_losses.pth.tar\")\n",
        "else:\n",
        "    model = load_model_from_checkpoint(\"/Users/matteom/shared-folder/artistic-colorization-UNet/Unet_adain/combined_losses_experiments/UNet-AdaIN_trsize5000_valsize500_best_model.pth.tar\")\n",
        "    model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Style analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_data_list = list(validation_data)\n",
        "validation_style_image_list = list(style_validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ground_truth = validation_data_list[2]['image']\n",
        "input_unet = validation_data_list[2]['grayscale_image']\n",
        "# input_encoder = validation_style_image_list[2]['image']   # To use an actual style image\n",
        "input_encoder = validation_data_list[2]['image']        # To use the image itself as style\n",
        "\n",
        "_, output = model(input_unet.unsqueeze(0).to(device), input_encoder.unsqueeze(0).to(device))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "\n",
        "plot_both(ground_truth.permute(1,2,0).numpy(),output.permute(1,2,0).numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## style is the image itself\n",
        "entry = training_data_list[32]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = entry['image']\n",
        "\n",
        "output = model(input_unet.unsqueeze(0).to(device), input_encoder.unsqueeze(0).to(device))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "plot_both(entry['image'].permute(1,2,0).numpy(),output.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## style is the style image\n",
        "entry = training_data_list[19]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = input_encoder_style\n",
        "input_encoder = entry['image']\n",
        "output = model(input_unet.unsqueeze(0).to(device), input_encoder.unsqueeze(0).to(device))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "plot_both(entry['image'].permute(1,2,0).numpy(),output.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reference_style = training_data_list[39]['image']\n",
        "plot_both(reference_style.permute(1,2,0).numpy(),reference_style.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colorization from LAB color space. \n",
        "\n",
        "Input is a HxW grayscale image (the l channel).\n",
        "Output is 2xHxW representing the ab channels.\n",
        "\n",
        "No std and mean normalization is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAIN_SIZE = 600\n",
        "# VAL_SIZE = 150\n",
        "# BATCH_SIZE = 16\n",
        "\n",
        "# train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace='LAB')\n",
        "# train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "# '''\n",
        "# Since the model is trained against 3XHxW RGB images I expect the order of the channels in the ouput to be the same\n",
        "# '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = UNet(in_C=1, out_C=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.train_model(train_loader=train_loader, val_loader=validation_loader, \n",
        "#                   epochs=55, \n",
        "#                   lr=0.0001, \n",
        "#                   loss_fn=nn.MSELoss(), \n",
        "#                   optimizer=torch.optim.Adam, \n",
        "#                   verbose=True, \n",
        "#                   save_path= 'experiment_results/LAB_55ep_600', \n",
        "#                   save_name_prefix='/1000_training_',\n",
        "#                   colorspace='LAB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[32],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[19],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[42],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[43],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## must have length 8!\n",
        "images_to_visualize_indexes = [3,12,5,17,11,3,28,29]\n",
        "\n",
        "visualization_images = get_visualization_images(images_to_visualize_indexes)\n",
        "outputs = get_batch_outputs(visualization_images)\n",
        "plot_grid(visualization_images, outputs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
