{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6CH_vkV8IEs",
        "outputId": "ccc64d9e-60d8-4718-b4f5-0a4a870b01ec"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from IPython.display import Markdown, display\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import dataset\n",
        "\n",
        "# sys.path.append(\"src/\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
        "                else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# print(torch.cuda.get_device_name(0))\n",
        "# sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_both(image1,image2):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot first image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image1)\n",
        "    plt.title('Image 1')\n",
        "\n",
        "    # Plot second image\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(image2)\n",
        "    plt.title('Image 2')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def compose_output(original_image, recreated_image):\n",
        "    print(original_image.shape)\n",
        "    print(recreated_image.shape)\n",
        "    l = original_image[0,:,:]\n",
        "    print(\"L shape\",l.shape)\n",
        "    return torch.cat((l.unsqueeze(0),recreated_image),0)\n",
        "\n",
        "'''\n",
        "TODO: when using lab color space we have certain artefacts that do not appear when we only use RGB images\n",
        "\n",
        "may be from normalization in the lab space or the transformation itself\n",
        "\n",
        "entry[image][0]\n",
        "'''\n",
        "def visualize(entry,model,lab=False):\n",
        "    image = entry['grayscale_image']\n",
        "    output = model(image.unsqueeze(0).to(device))\n",
        "    output = output.detach().cpu().squeeze(0)\n",
        "\n",
        "    if lab:\n",
        "        lab2rgb = dataset.LABtoRGB()\n",
        "        rgb_image = lab2rgb(entry['image'])\n",
        "        ## TODO: if here entry['image'][0:1,:,:] is used, the image is not displayed correctly\n",
        "        composed_lab = torch.cat((entry['grayscale_image'],output),0)\n",
        "        composed_output = lab2rgb(composed_lab)\n",
        "\n",
        "\n",
        "    plot_both(rgb_image.permute(1,2,0).numpy(),composed_output.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_grid(images_column1, images_column2):\n",
        "    '''\n",
        "    Plots a 2x8 grid of rgb images, where each row corresponds to a pair of images from the two input lists.\n",
        "    '''\n",
        "    num_images_column1 = len(images_column1)\n",
        "    num_images_column2 = len(images_column2)\n",
        "    num_rows = max(num_images_column1, num_images_column2)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))  # Adjust figsize as needed\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        # Plot images from the first column\n",
        "        if i < num_images_column1:\n",
        "            plt.subplot(num_rows, 2, 2*i + 1)\n",
        "            plt.imshow(images_column1[i].permute(1,2,0).numpy())\n",
        "            # plt.title(f'Image {i + 1} (Column 1)')\n",
        "            plt.axis('off')  # Turn off axis for better visualization\n",
        "        \n",
        "        # Plot images from the second column\n",
        "        if i < num_images_column2:\n",
        "            plt.subplot(num_rows, 2, 2*i + 2)\n",
        "            plt.imshow(images_column2[i].permute(1,2,0).numpy())\n",
        "            # plt.title(f'Image {i + 1} (Column 2)')\n",
        "            plt.axis('off')  # Turn off axis for better visualization\n",
        "\n",
        "    plt.tight_layout(pad=0.1)  # Adjust spacing between subplots\n",
        "    plt.show()\n",
        "\n",
        "def get_visualization_images(indexes):\n",
        "    data = list(train_data)\n",
        "    tbr = []\n",
        "    for idx in indexes:\n",
        "        tbr.append(data[idx]['image'])\n",
        "\n",
        "    return tbr\n",
        "\n",
        "def get_batch_outputs(images):\n",
        "    ## batch the images\n",
        "    images = torch.stack(images)\n",
        "    output = net(images.to(device))\n",
        "    return [output[i].detach().cpu() for i in range(output.shape[0])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Adain_Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_dim):\n",
        "        super(Adain_Encoder, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, out_dim, kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = torch.relu(self.conv4(x))\n",
        "        x = torch.relu(self.conv5(x))\n",
        "        \n",
        "        # Global average pooling\n",
        "        x = self.global_avg_pool(x)        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# Define input and output dimensions\n",
        "in_channels = 3  # number of input channels\n",
        "out_dim = 128  # size of the output representation\n",
        "\n",
        "# Create an instance of the ConvNet\n",
        "model = Adain_Encoder(in_channels, out_dim)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(4, in_channels, 256, 256)\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    \n",
        "    def __init__(self, style_dim, channels):\n",
        "        super(AdaIN, self).__init__()\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
        "        self.style_scale_transform = nn.Linear(style_dim, channels)\n",
        "        self.style_shift_transform = nn.Linear(style_dim, channels)\n",
        "\n",
        "        ## to ensure they learn different stuff | How tho?\n",
        "        init.normal_(self.style_scale_transform.weight, mean=1.0, std=0.02)\n",
        "        init.normal_(self.style_shift_transform.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        self.style_scale_transform.bias.data.fill_(1)  # Initialize scale to 1\n",
        "        self.style_shift_transform.bias.data.fill_(0)  # Initialize shift to 0\n",
        "\n",
        "    def forward(self, x, style):\n",
        "        '''\n",
        "        x - feature maps from the unet\n",
        "        y - learned (jointly) from encoder\n",
        "\n",
        "        return:\n",
        "        same size as x\n",
        "        '''\n",
        "        # Normalize the input feature map\n",
        "        normalized = self.instance_norm(x)\n",
        "        \n",
        "        # Extract style scale and shift parameters from the style vector\n",
        "        scale = self.style_scale_transform(style)[:, :, None, None]\n",
        "        shift = self.style_shift_transform(style)[:, :, None, None]\n",
        "\n",
        "        # print(scale.squeeze()[:10])\n",
        "        # print(shift.squeeze()[:10])\n",
        "        \n",
        "        # Apply scale and shift to the normalized feature map\n",
        "        transformed = scale * normalized + shift\n",
        "        \n",
        "        return transformed\n",
        "\n",
        "# Example usage:\n",
        "# Define the dimensions\n",
        "style_dim = 128  # dimensionality of the style vector\n",
        "channels = 256    # number of channels in the feature map\n",
        "\n",
        "# Create an instance of AdaIN\n",
        "adain = AdaIN(style_dim, channels)\n",
        "\n",
        "# Generate random input feature map and style vector\n",
        "x = torch.randn(4, channels, 64, 64)  # Example input feature map\n",
        "style = torch.randn(1, style_dim)     # Example style vector\n",
        "\n",
        "# Apply AdaIN\n",
        "\n",
        "output = adain(x, style)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq2e0mN_8IEw"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_C = 3, out_C=3):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_C, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.maxpool_1to2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.maxpool_2to3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.maxpool_3to4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "        self.maxpool_4to5 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.conv_transpose_5to6 = nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_5to6 = nn.Conv2d(1024, 512, 1)\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.conv_transpose_6to7 = nn.ConvTranspose2d(256, 256, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_6to7 = nn.Conv2d(512, 256, 1)\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.conv_transpose_7to8 = nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_7to8 = nn.Conv2d(256, 128, 1)\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv_transpose8to9 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_8to9 = nn.Conv2d(128, 64, 1)\n",
        "        self.conv9 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, padding=1),      # Simmetry broken here: keeps being 64 (from paper)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv10 = nn.Conv2d(64, out_C, 1)\n",
        "\n",
        "        '''\n",
        "        Conv1 shape torch.Size([16, 64, 128, 128])\n",
        "        Conv2 shape torch.Size([16, 128, 64, 64])\n",
        "        Conv3 shape torch.Size([16, 256, 32, 32])\n",
        "        Conv4 shape torch.Size([16, 512, 16, 16])\n",
        "        Conv5 shape torch.Size([16, 512, 8, 8])\n",
        "        '''\n",
        "\n",
        "        ## we need adain layers defined for each number of channels of the feature maps\n",
        "        ## NOTE: adain2 learns a non linear mapping in the same dimensional space, does it matter?\n",
        "        self.encoder_adain1 = AdaIN(128, 64)\n",
        "        self.encoder_adain2 = AdaIN(128, 128)\n",
        "        self.encoder_adain3 = AdaIN(128, 256)\n",
        "        self.encoder_adain4 = AdaIN(128, 512)\n",
        "        self.encoder_adain5 = AdaIN(128, 512)\n",
        "\n",
        "        self.decoder_adain4 = AdaIN(128, 512)\n",
        "        self.decoder_adain3 = AdaIN(128, 256)\n",
        "        self.decoder_adain2 = AdaIN(128, 128)\n",
        "        self.decoder_adain1 = AdaIN(128, 64)\n",
        "\n",
        "\n",
        "    def forward(self,x,style):\n",
        "        x=x.to(device)\n",
        "        style = style.to(device)\n",
        "\n",
        "        # Encoder\n",
        "        conv1 = self.conv1(x)\n",
        "        conv1 = self.encoder_adain1(x,style) ## AdaIN\n",
        "\n",
        "        maxpooled_1to2 = self.maxpool_1to2(conv1)\n",
        "        conv2 = self.conv2(maxpooled_1to2)\n",
        "        conv2 = self.encoder_adain2(conv2,style) ## AdaIN\n",
        "\n",
        "        maxpooled_2to3 = self.maxpool_2to3(conv2)\n",
        "        conv3 = self.conv3(maxpooled_2to3)\n",
        "        conv3 = self.encoder_adain3(conv3,style) ## AdaIN\n",
        "\n",
        "        maxpooled_3to4 = self.maxpool_3to4(conv3)\n",
        "        conv4 = self.conv4(maxpooled_3to4)\n",
        "        conv4 = self.encoder_adain4(conv4,style) ## AdaIN\n",
        "\n",
        "        maxpooled_4to5 = self.maxpool_4to5(conv4)\n",
        "        conv5 = self.conv5(maxpooled_4to5)\n",
        "        conv5 = self.encoder_adain5(conv5,style) ## AdaIN\n",
        "        # Decoder\n",
        "        concatenation_5to6 = torch.cat((conv4,self.decoder_adain4(self.conv_transpose_5to6(conv5),style)),1)\n",
        "        # concatenation_5to6 = torch.cat((conv4,self.conv_transpose_5to6(conv5)),1)\n",
        "        skip_fusion_5to6 = self.conv1d_fusing_5to6(concatenation_5to6)\n",
        "        conv6 = self.conv6(skip_fusion_5to6)\n",
        "\n",
        "        concatenation_6to7 = torch.cat((conv3, self.decoder_adain3(self.conv_transpose_6to7(conv6),style)),1)\n",
        "        # concatenation_6to7 = torch.cat((conv3, self.conv_transpose_6to7(conv6)),1)\n",
        "        skip_fusion_6to7 = self.conv1d_fusing_6to7(concatenation_6to7)\n",
        "        conv7 = self.conv7(skip_fusion_6to7)\n",
        "\n",
        "        concatenation_7to8 = torch.cat((conv2, self.decoder_adain2(self.conv_transpose_7to8(conv7),style)),1)\n",
        "        # concatenation_7to8 = torch.cat((conv2, self.conv_transpose_7to8(conv7)),1)\n",
        "        skip_fusion_7to8 = self.conv1d_fusing_7to8(concatenation_7to8)\n",
        "        conv8 = self.conv8(skip_fusion_7to8)\n",
        "\n",
        "        concatenation_8_to9 = torch.cat((conv1, self.decoder_adain1(self.conv_transpose8to9(conv8),style)),1)\n",
        "        # concatenation_8_to9 = torch.cat((conv1, self.conv_transpose8to9(conv8)),1)\n",
        "        skip_fusion_8to9 = self.conv1d_fusing_8to9(concatenation_8_to9)\n",
        "        conv9 = self.conv9(skip_fusion_8to9)\n",
        "\n",
        "        output = self.conv10(conv9)\n",
        "        return output\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, \n",
        "                    epochs=10, \n",
        "                    lr=0.001, \n",
        "                    loss_fn=nn.MSELoss(), \n",
        "                    optimizer=torch.optim.Adam, \n",
        "                    verbose=True, \n",
        "                    save_path = \"./model_storage/\", \n",
        "                    save_name_prefix='/',\n",
        "                    colorspace='RGB'):\n",
        "\n",
        "        '''\n",
        "        save_path = folder for saving results\n",
        "        save_name_prefix = prefix for saved files to mark experiments (start with '/')\n",
        "\n",
        "\n",
        "        '''\n",
        "\n",
        "\n",
        "        loss_df = pd.DataFrame(['training_loss', 'validation_loss'])\n",
        "        self.to(device)\n",
        "        self.train()\n",
        "        optimizer = optimizer(self.parameters(), lr=lr)\n",
        "\n",
        "        #for saving progress\n",
        "        best_val_loss = 99999\n",
        "        val_loss_list = []\n",
        "        training_loss_list = []\n",
        "\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            training_loss = 0\n",
        "            for i, batch_data in enumerate(train_loader):\n",
        "                image = batch_data['grayscale_image']\n",
        "                image = image.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                reproduced_image = self(image)\n",
        "                reproduced_image = reproduced_image.to(device)\n",
        "                ## aici trebuie comparat doar cu canalele ab\n",
        "                if colorspace == 'RGB':\n",
        "                    loss = loss_fn(reproduced_image, batch_data['image'].to(device))\n",
        "                elif colorspace == 'LAB':\n",
        "                    loss = loss_fn(reproduced_image, batch_data['image'][:,1:,:,:].to(device))\n",
        "                training_loss += loss.item()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            #average training loss\n",
        "            training_loss /= len(train_loader)\n",
        "            training_loss_list.append(training_loss)\n",
        "\n",
        "\n",
        "            if (epoch)%5 == 0 or (epoch + 1) == epochs:\n",
        "                val_loss = 0.0  # Track validation loss\n",
        "                with torch.no_grad():  # Disable gradient calculation for validation\n",
        "                    for val_data in val_loader:\n",
        "                        val_image = val_data['grayscale_image']\n",
        "                        val_image = val_image.to(device)\n",
        "\n",
        "                        val_reproduced_image = self(val_image)\n",
        "                        val_reproduced_image = val_reproduced_image.to(device)\n",
        "                        if colorspace == 'RGB':\n",
        "                            val_loss += loss_fn(val_reproduced_image, val_data['image'].to(device)).item()\n",
        "                        elif colorspace == 'LAB':\n",
        "                            val_loss += loss_fn(val_reproduced_image, val_data['image'][:,1:,:,:].to(device)).item()\n",
        "                       \n",
        "                val_loss /= len(val_loader)  # Calculate average validation loss\n",
        "                val_loss_list.append(val_loss)\n",
        "\n",
        "                if verbose:\n",
        "                  print(f\"Epoch {epoch}, training_loss = {training_loss}\")\n",
        "                  print(f\"Epoch {epoch}: validation loss = {val_loss}\")\n",
        "\n",
        "\n",
        "                if val_loss < best_val_loss:  # Update best validation loss and save checkpoint if best model\n",
        "                  state_diction = self.state_dict()\n",
        "                  best_val_loss = val_loss\n",
        "                  for key in state_diction.keys():\n",
        "                      state_diction[key] = state_diction[key].to(torch.device('cpu'))\n",
        "                  torch.save(state_diction, (save_path+save_name_prefix+f\"UNet_best_model.pth.tar\"))\n",
        "\n",
        "                #construct df with losses\n",
        "                loss_df = pd.DataFrame({'epoch': range(0,epoch+1),  'training_loss':training_loss_list, 'val_loss': val_loss_list})\n",
        "                loss_df.to_csv(save_path+save_name_prefix+\"UNet_loss.csv\", index=False)\n",
        "                # plot losses\n",
        "                plt.plot(loss_df['epoch'], loss_df['training_loss'], label='Training Loss')\n",
        "                plt.plot(loss_df['epoch'], loss_df['val_loss'].interpolate(method='linear'), label='Validation Loss')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.title('Training and Validation Loss')\n",
        "                plt.legend()\n",
        "\n",
        "                # Save plot as .png file\n",
        "                plt.savefig(save_path+save_name_prefix+'UNet_loss_plot.png')\n",
        "                plt.clf()\n",
        "            else:\n",
        "              val_loss_list.append(np.nan)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colorization from RGB color space. \n",
        "\n",
        "Input is a HxW grayscale image.\n",
        "Output is a 3xHxW RGB image.\n",
        "\n",
        "No std and mean normalization is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_SIZE = 400\n",
        "VAL_SIZE = 100\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace='RGB')\n",
        "train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "'''\n",
        "Since the model is trained against 3XHxW RGB images I expect the order of the channels in the ouput to be the same\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO_EPOCHS = 45\n",
        "\n",
        "out_dim = 128  # size of the output representation\n",
        "\n",
        "model = UNet(in_C=1, out_C=3).to(device)\n",
        "style_encoder = Adain_Encoder(3, out_dim).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "optimizer_style = torch.optim.Adam(style_encoder.parameters(), lr=0.001)\n",
        "loss_reconstruction = nn.MSELoss()\n",
        "\n",
        "model.train()\n",
        "style_encoder.train()\n",
        "'''\n",
        "Assume RGB colorspace\n",
        "'''\n",
        "training_loss_list = []\n",
        "\n",
        "for epoch in tqdm(range(NO_EPOCHS)):\n",
        "    training_loss = 0\n",
        "    for batch_data in train_loader:\n",
        "        input_unet = batch_data['grayscale_image']\n",
        "        input_unet = input_unet.to(device)\n",
        "        \n",
        "        input_encoder = batch_data['image']\n",
        "        input_encoder = input_encoder.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_style.zero_grad()\n",
        "\n",
        "        encoded_style = style_encoder(input_encoder)\n",
        "        \n",
        "        reproduced_image = model(input_unet, encoded_style)\n",
        "        reproduced_image = reproduced_image.to(device)\n",
        "       \n",
        "        '''\n",
        "        What loss to propagate to the encoder network?\n",
        "        '''\n",
        "        combined_loss = loss_reconstruction(reproduced_image, batch_data['image'].to(device))\n",
        "\n",
        "        training_loss += combined_loss.item()\n",
        "        combined_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer_style.step()\n",
        "\n",
        "    training_loss /= len(train_loader)\n",
        "    training_loss_list.append(training_loss)\n",
        "    print(f\"Epoch {epoch}, training_loss = {training_loss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Style analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_data_list = list(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_encoder_style = training_data_list[38]['image']\n",
        "\n",
        "## plot reference style\n",
        "plot_both(input_encoder_style.permute(1,2,0).numpy(),input_encoder_style.permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## style is the image itself\n",
        "entry = training_data_list[32]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = entry['image']\n",
        "print(input_encoder.shape)\n",
        "output = model(input_unet.unsqueeze(0).to(device), style_encoder(input_encoder.unsqueeze(0).to(device)))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "plot_both(input_encoder.permute(1,2,0).numpy(),output.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Changed style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "entry = training_data_list[32]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = input_encoder_style\n",
        "print(input_encoder.shape)\n",
        "output = model(input_unet.unsqueeze(0).to(device), style_encoder(input_encoder.unsqueeze(0).to(device)))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "plot_both(input_encoder.permute(1,2,0).numpy(),output.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reference_style = training_data_list[39]['image']\n",
        "plot_both(reference_style.permute(1,2,0).numpy(),reference_style.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "sm1i_1Dv8IEx",
        "outputId": "48d07cdb-2739-4056-9ca2-616359657573"
      },
      "outputs": [],
      "source": [
        "model.train_model(train_loader=train_loader, val_loader=validation_loader, epochs=10, lr=0.0001, loss_fn=nn.MSELoss(), optimizer=torch.optim.Adam, verbose=True, save_path= 'experiment_results/LAB_100', save_name_prefix='/1000_training_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize(list(train_data)[32],model,lab=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colorization from LAB color space. \n",
        "\n",
        "Input is a HxW grayscale image (the l channel).\n",
        "Output is 2xHxW representing the ab channels.\n",
        "\n",
        "No std and mean normalization is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_SIZE = 600\n",
        "VAL_SIZE = 150\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace='LAB')\n",
        "train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "'''\n",
        "Since the model is trained against 3XHxW RGB images I expect the order of the channels in the ouput to be the same\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = UNet(in_C=1, out_C=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.train_model(train_loader=train_loader, val_loader=validation_loader, \n",
        "                  epochs=55, \n",
        "                  lr=0.0001, \n",
        "                  loss_fn=nn.MSELoss(), \n",
        "                  optimizer=torch.optim.Adam, \n",
        "                  verbose=True, \n",
        "                  save_path= 'experiment_results/LAB_55ep_600', \n",
        "                  save_name_prefix='/1000_training_',\n",
        "                  colorspace='LAB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[32],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[19],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[42],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[43],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## must have length 8!\n",
        "images_to_visualize_indexes = [3,12,5,17,11,3,28,29]\n",
        "\n",
        "visualization_images = get_visualization_images(images_to_visualize_indexes)\n",
        "outputs = get_batch_outputs(visualization_images)\n",
        "plot_grid(visualization_images, outputs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
