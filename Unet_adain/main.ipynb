{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6CH_vkV8IEs",
        "outputId": "ccc64d9e-60d8-4718-b4f5-0a4a870b01ec"
      },
      "outputs": [],
      "source": [
        "import dataset\n",
        "from visualize import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: use this loss as style loss\n",
        "\n",
        "import lpips\n",
        "perceptual_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "# d = loss_fn.forward(im0,im1)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Adain_Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_dim):\n",
        "        super(Adain_Encoder, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, out_dim, kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = torch.relu(self.conv4(x))\n",
        "        x = torch.relu(self.conv5(x))\n",
        "        \n",
        "        # Global average pooling\n",
        "        x = self.global_avg_pool(x)        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# Define input and output dimensions\n",
        "in_channels = 3  # number of input channels\n",
        "out_dim = 128  # size of the output representation\n",
        "\n",
        "# Create an instance of the ConvNet\n",
        "model = Adain_Encoder(in_channels, out_dim)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(4, in_channels, 256, 256)\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "class AdaIN_interface(nn.Module):\n",
        "    \n",
        "    def __init__(self, style_dim, channels):\n",
        "        super(AdaIN_interface, self).__init__()\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
        "        self.style_scale_transform = nn.Linear(style_dim, channels)\n",
        "        self.style_shift_transform = nn.Linear(style_dim, channels)\n",
        "\n",
        "        ## to ensure they learn different stuff | How tho?\n",
        "        init.normal_(self.style_scale_transform.weight, mean=1.0, std=0.02)\n",
        "        init.normal_(self.style_shift_transform.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        self.style_scale_transform.bias.data.fill_(1)  # Initialize scale to 1\n",
        "        self.style_shift_transform.bias.data.fill_(0)  # Initialize shift to 0\n",
        "\n",
        "    def forward(self, x, encoded_style):\n",
        "        '''\n",
        "        x - feature maps from the unet\n",
        "        y - learned (jointly) from encoder\n",
        "\n",
        "        return:\n",
        "        same size as x\n",
        "        '''\n",
        "        # Normalize the input feature map\n",
        "        normalized = self.instance_norm(x)\n",
        "        \n",
        "        # Extract style scale and shift parameters from the style vector\n",
        "        scale = self.style_scale_transform(style)[:, :, None, None]\n",
        "        shift = self.style_shift_transform(style)[:, :, None, None]\n",
        "        \n",
        "        # Apply scale and shift to the normalized feature map\n",
        "        transformed = scale * normalized + shift\n",
        "        \n",
        "        return transformed\n",
        "\n",
        "# Example usage:\n",
        "# Define the dimensions\n",
        "style_dim = 128  # dimensionality of the style vector\n",
        "channels = 256    # number of channels in the feature map\n",
        "\n",
        "# Create an instance of AdaIN\n",
        "adain = AdaIN_interface(style_dim, channels)\n",
        "\n",
        "# Generate random input feature map and style vector\n",
        "x = torch.randn(4, channels, 64, 64)  # Example input feature map\n",
        "style = torch.randn(1, style_dim)     # Example style vector\n",
        "\n",
        "# Apply AdaIN\n",
        "\n",
        "output = adain(x, style)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq2e0mN_8IEw"
      },
      "outputs": [],
      "source": [
        "OUT_VEC_DIM = 128\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_C = 3, out_C=3, style_dim=128):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_C, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.maxpool_1to2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.maxpool_2to3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.maxpool_3to4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "        self.maxpool_4to5 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.conv_transpose_5to6 = nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_5to6 = nn.Conv2d(1024, 512, 1)\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.conv_transpose_6to7 = nn.ConvTranspose2d(256, 256, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_6to7 = nn.Conv2d(512, 256, 1)\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.conv_transpose_7to8 = nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_7to8 = nn.Conv2d(256, 128, 1)\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv_transpose8to9 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_8to9 = nn.Conv2d(128, 64, 1)\n",
        "        self.conv9 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, padding=1),      # Simmetry broken here: keeps being 64 (from paper)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv10 = nn.Conv2d(64, out_C, 1)\n",
        "\n",
        "        '''\n",
        "        Conv1 shape torch.Size([16, 64, 128, 128])\n",
        "        Conv2 shape torch.Size([16, 128, 64, 64])\n",
        "        Conv3 shape torch.Size([16, 256, 32, 32])\n",
        "        Conv4 shape torch.Size([16, 512, 16, 16])\n",
        "        Conv5 shape torch.Size([16, 512, 8, 8])\n",
        "        '''\n",
        "\n",
        "        self.encoder_adain1 = AdaIN(128, 64)\n",
        "        self.encoder_adain2 = AdaIN(128, 128)\n",
        "        self.encoder_adain3 = AdaIN(128, 256)\n",
        "        self.encoder_adain4 = AdaIN(128, 512)\n",
        "        self.encoder_adain5 = AdaIN(128, 512)\n",
        "\n",
        "        # AdaIN interface layers\n",
        "        ## Encoder side\n",
        "        self.encoder_adain1 = AdaIN_interface(OUT_VEC_DIM, 64)\n",
        "        self.encoder_adain2 = AdaIN_interface(OUT_VEC_DIM, 128)\n",
        "        self.encoder_adain3 = AdaIN_interface(OUT_VEC_DIM, 256)\n",
        "        self.encoder_adain4 = AdaIN_interface(OUT_VEC_DIM, 512)\n",
        "        ## Bottleneck\n",
        "        self.encoder_adain5 = AdaIN_interface(OUT_VEC_DIM, 512)\n",
        "        ## Decoder side\n",
        "        self.decoder_adain4 = AdaIN_interface(OUT_VEC_DIM, 512)\n",
        "        self.decoder_adain3 = AdaIN_interface(OUT_VEC_DIM, 256)\n",
        "        self.decoder_adain2 = AdaIN_interface(OUT_VEC_DIM, 128)\n",
        "        self.decoder_adain1 = AdaIN_interface(OUT_VEC_DIM, 64)\n",
        "\n",
        "        if out_C == 3:\n",
        "            self.style_encoder = Adain_Encoder(in_channels=3, out_dim=128)\n",
        "        elif out_C == 2:\n",
        "            self.style_encoder = Adain_Encoder(in_channels=2, out_dim=128)\n",
        "\n",
        "\n",
        "    def forward(self,x,style_image):\n",
        "        x=x.to(device)\n",
        "        style_image = style_image.to(device)\n",
        "\n",
        "        style = self.style_encoder(style_image).to(device)\n",
        "\n",
        "        # Encoder\n",
        "        conv1 = self.conv1(x)\n",
        "        conv1 = self.encoder_adain1(x,encoded_style) ## AdaIN normalization applied\n",
        "\n",
        "        maxpooled_1to2 = self.maxpool_1to2(conv1)\n",
        "        conv2 = self.conv2(maxpooled_1to2)\n",
        "        conv2 = self.encoder_adain2(conv2,encoded_style) ## AdaIN normalization applied\n",
        "\n",
        "        maxpooled_2to3 = self.maxpool_2to3(conv2)\n",
        "        conv3 = self.conv3(maxpooled_2to3)\n",
        "        conv3 = self.encoder_adain3(conv3,encoded_style) ## AdaIN normalization applied\n",
        "\n",
        "        maxpooled_3to4 = self.maxpool_3to4(conv3)\n",
        "        conv4 = self.conv4(maxpooled_3to4)\n",
        "        conv4 = self.encoder_adain4(conv4,encoded_style) ## AdaIN normalization applied\n",
        "\n",
        "        maxpooled_4to5 = self.maxpool_4to5(conv4)\n",
        "        conv5 = self.conv5(maxpooled_4to5)\n",
        "        conv5 = self.encoder_adain5(conv5,encoded_style) ## AdaIN normalization applied\n",
        "\n",
        "        # Decoder\n",
        "        concatenation_5to6 = torch.cat((conv4,self.decoder_adain4(self.conv_transpose_5to6(conv5),style)),1)\n",
        "        # concatenation_5to6 = torch.cat((conv4,self.conv_transpose_5to6(conv5)),1)\n",
        "        skip_fusion_5to6 = self.conv1d_fusing_5to6(concatenation_5to6)\n",
        "        conv6 = self.conv6(skip_fusion_5to6)\n",
        "\n",
        "        concatenation_6to7 = torch.cat((conv3, self.decoder_adain3(self.conv_transpose_6to7(conv6),style)),1)\n",
        "        # concatenation_6to7 = torch.cat((conv3, self.conv_transpose_6to7(conv6)),1)\n",
        "        skip_fusion_6to7 = self.conv1d_fusing_6to7(concatenation_6to7)\n",
        "        conv7 = self.conv7(skip_fusion_6to7)\n",
        "\n",
        "        concatenation_7to8 = torch.cat((conv2, self.decoder_adain2(self.conv_transpose_7to8(conv7),style)),1)\n",
        "        # concatenation_7to8 = torch.cat((conv2, self.conv_transpose_7to8(conv7)),1)\n",
        "        skip_fusion_7to8 = self.conv1d_fusing_7to8(concatenation_7to8)\n",
        "        conv8 = self.conv8(skip_fusion_7to8)\n",
        "\n",
        "        concatenation_8_to9 = torch.cat((conv1, self.decoder_adain1(self.conv_transpose8to9(conv8),style)),1)\n",
        "        # concatenation_8_to9 = torch.cat((conv1, self.conv_transpose8to9(conv8)),1)\n",
        "        skip_fusion_8to9 = self.conv1d_fusing_8to9(concatenation_8_to9)\n",
        "        conv9 = self.conv9(skip_fusion_8to9)\n",
        "    \n",
        "\n",
        "        output = self.conv10(conv9)\n",
        "        return output\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, \n",
        "                    epochs=54, \n",
        "                    lr=0.0001, \n",
        "                    loss_fn=nn.MSELoss(), \n",
        "                    optimizer=torch.optim.Adam, \n",
        "                    verbose=True, \n",
        "                    save_path = \"./model_storage/\", \n",
        "                    save_name_prefix='/',\n",
        "                    colorspace='RGB'):\n",
        "\n",
        "        '''\n",
        "        save_path = folder for saving results\n",
        "        save_name_prefix = prefix for saved files to mark experiments (start with '/')\n",
        "\n",
        "\n",
        "        '''\n",
        "\n",
        "\n",
        "        loss_df = pd.DataFrame(['training_loss', 'validation_loss'])\n",
        "        self.to(device)\n",
        "        self.train()\n",
        "        optimizer = optimizer(self.parameters(), lr=lr)\n",
        "\n",
        "        #for saving progress\n",
        "        best_val_loss = 99999\n",
        "        val_loss_list = []\n",
        "        training_loss_list = []\n",
        "\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            training_loss = 0\n",
        "            for i, batch_data in enumerate(train_loader):\n",
        "                image = batch_data['grayscale_image']\n",
        "                image = image.to(device)\n",
        "    \n",
        "                input_encoder = batch_data['image']\n",
        "                input_encoder = input_encoder.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "    \n",
        "                reproduced_image = self(image,input_encoder)\n",
        "                reproduced_image = reproduced_image.to(device)\n",
        "                ## aici trebuie comparat doar cu canalele ab\n",
        "                if colorspace == 'RGB':\n",
        "                    loss = loss_fn(reproduced_image, batch_data['image'].to(device))\n",
        "                elif colorspace == 'LAB':\n",
        "                    loss = loss_fn(reproduced_image, batch_data['image'][:,1:,:,:].to(device))\n",
        "                training_loss += loss.item()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            #average training loss\n",
        "            training_loss /= len(train_loader)\n",
        "            training_loss_list.append(training_loss)\n",
        "\n",
        "\n",
        "            if (epoch)%5 == 0 or (epoch + 1) == epochs:\n",
        "                val_loss = 0.0  # Track validation loss\n",
        "                with torch.no_grad():  # Disable gradient calculation for validation\n",
        "                    for val_data in val_loader:\n",
        "                        val_image = val_data['grayscale_image']\n",
        "                        val_image = val_image.to(device)\n",
        "\n",
        "                        input_encoder = val_data['image']\n",
        "                        input_encoder = input_encoder.to(device)\n",
        "\n",
        "\n",
        "                        val_reproduced_image = self(val_image,input_encoder)\n",
        "                        val_reproduced_image = val_reproduced_image.to(device)\n",
        "                        if colorspace == 'RGB':\n",
        "                            val_loss += loss_fn(val_reproduced_image, val_data['image'].to(device)).item()\n",
        "                        elif colorspace == 'LAB':\n",
        "                            val_loss += loss_fn(val_reproduced_image, val_data['image'][:,1:,:,:].to(device)).item()\n",
        "                       \n",
        "                val_loss /= len(val_loader)  # Calculate average validation loss\n",
        "                val_loss_list.append(val_loss)\n",
        "\n",
        "                if verbose:\n",
        "                  print(f\"Epoch {epoch}, training_loss = {training_loss}\")\n",
        "                  print(f\"Epoch {epoch}: validation loss = {val_loss}\")\n",
        "\n",
        "\n",
        "                if val_loss < best_val_loss:  # Update best validation loss and save checkpoint if best model\n",
        "                  state_diction = self.state_dict()\n",
        "                  best_val_loss = val_loss\n",
        "                  for key in state_diction.keys():\n",
        "                      state_diction[key] = state_diction[key].to(torch.device('cpu'))\n",
        "                  torch.save(state_diction, (save_path+save_name_prefix+f\"UNet_best_model.pth.tar\"))\n",
        "\n",
        "                #construct df with losses\n",
        "                loss_df = pd.DataFrame({'epoch': range(0,epoch+1),  'training_loss':training_loss_list, 'val_loss': val_loss_list})\n",
        "                loss_df.to_csv(save_path+save_name_prefix+\"UNet_loss.csv\", index=False)\n",
        "                # plot losses\n",
        "                plt.plot(loss_df['epoch'], loss_df['training_loss'], label='Training Loss')\n",
        "                plt.plot(loss_df['epoch'], loss_df['val_loss'].interpolate(method='linear'), label='Validation Loss')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.title('Training and Validation Loss')\n",
        "                plt.legend()\n",
        "\n",
        "                # Save plot as .png file\n",
        "                plt.savefig(save_path+save_name_prefix+'UNet_loss_plot.png')\n",
        "                plt.clf()\n",
        "            else:\n",
        "              val_loss_list.append(np.nan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_from_checkpoint(checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        # Load the checkpoint\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        \n",
        "        # Create model architecture\n",
        "        # For example, if your model class is named MyModel:\n",
        "        model = UNet(in_C=1, out_C=3)\n",
        "        \n",
        "        # Load state_dict into the model\n",
        "        model.load_state_dict(checkpoint)\n",
        "        \n",
        "        # Optionally, load other elements from the checkpoint such as optimizer state, etc.\n",
        "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        \n",
        "        print(\"Model loaded successfully from checkpoint.\")\n",
        "        return model.to(device)\n",
        "    else:\n",
        "        print(\"Checkpoint file does not exist.\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse_loss_fn = nn.MSELoss()\n",
        "def perceptual_and_MSE_loss(reproduced_image, original_image):\n",
        "    '''\n",
        "    reproduced_image: output of the model\n",
        "    original_image: ground truth\n",
        "    '''\n",
        "    # Perceptual loss\n",
        "    perceptual_loss = perceptual_loss_fn(reproduced_image, original_image).mean()\n",
        "    \n",
        "    # MSE loss\n",
        "    mse_loss = mse_loss_fn(reproduced_image, original_image)\n",
        "    \n",
        "    return perceptual_loss + mse_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colorization from RGB color space. \n",
        "\n",
        "Input is a HxW grayscale image.\n",
        "Output is a 3xHxW RGB image.\n",
        "\n",
        "No std and mean normalization is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_SIZE = 100\n",
        "VAL_SIZE = 150\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace='RGB',resolution=(128,128))\n",
        "train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "'''\n",
        "Since the model is trained against 3XHxW RGB images I expect the order of the channels in the ouput to be the same\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Device is: \", device)\n",
        "model = UNet(in_C=1, out_C=3).to(device)\n",
        "\n",
        "if TRAIN:\n",
        "    model.train_model(train_loader=train_loader, val_loader=validation_loader, epochs=54, lr=0.0001, loss_fn=perceptual_and_MSE_loss, optimizer=torch.optim.Adam, verbose=True, save_path= 'combined_losses_experiments/', save_name_prefix='/unet')\n",
        "    state_diction = model.state_dict()\n",
        "\n",
        "    for key in state_diction.keys():\n",
        "        state_diction[key] = state_diction[key].to(torch.device('cpu'))\n",
        "\n",
        "    torch.save(state_diction, \"unet_adain_combined_losses.pth.tar\")\n",
        "else:\n",
        "    model = load_model_from_checkpoint(\"combined_losses_experiments/unetUNet_best_model.pth.tar\")\n",
        "    model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Style analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_data_list = list(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_encoder_style = training_data_list[38]['image']\n",
        "\n",
        "## plot reference style\n",
        "plot_both(input_encoder_style.permute(1,2,0).numpy(),input_encoder_style.permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## style is the image itself\n",
        "entry = training_data_list[39]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = entry['image']\n",
        "\n",
        "output = model(input_unet.unsqueeze(0).to(device), input_encoder.unsqueeze(0).to(device))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "# plot_both(input_encoder.permute(1,2,0).numpy(),output.permute(1,2,0).numpy(),save_name=\"results/butterfly.png\")\n",
        "\n",
        "plot_both(input_encoder.permute(1,2,0).numpy(),output.permute(1,2,0).numpy(),save_name=\"results_combined_loss/leopard_no_style.png\")\n",
        "\n",
        "# plot_both(input_encoder.permute(1,2,0).numpy(),output.permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## style is the image itself\n",
        "entry = training_data_list[32]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = entry['image']\n",
        "\n",
        "output = model(input_unet.unsqueeze(0).to(device), input_encoder.unsqueeze(0).to(device))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "plot_both(entry['image'].permute(1,2,0).numpy(),output.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## style is the style image\n",
        "entry = training_data_list[19]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = input_encoder_style\n",
        "input_encoder = entry['image']\n",
        "output = model(input_unet.unsqueeze(0).to(device), input_encoder.unsqueeze(0).to(device))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "plot_both(entry['image'].permute(1,2,0).numpy(),output.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reference_style = training_data_list[39]['image']\n",
        "plot_both(reference_style.permute(1,2,0).numpy(),reference_style.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colorization from LAB color space. \n",
        "\n",
        "Input is a HxW grayscale image (the l channel).\n",
        "Output is 2xHxW representing the ab channels.\n",
        "\n",
        "No std and mean normalization is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAIN_SIZE = 600\n",
        "# VAL_SIZE = 150\n",
        "# BATCH_SIZE = 16\n",
        "\n",
        "# train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace='LAB')\n",
        "# train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "# '''\n",
        "# Since the model is trained against 3XHxW RGB images I expect the order of the channels in the ouput to be the same\n",
        "# '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = UNet(in_C=1, out_C=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.train_model(train_loader=train_loader, val_loader=validation_loader, \n",
        "#                   epochs=55, \n",
        "#                   lr=0.0001, \n",
        "#                   loss_fn=nn.MSELoss(), \n",
        "#                   optimizer=torch.optim.Adam, \n",
        "#                   verbose=True, \n",
        "#                   save_path= 'experiment_results/LAB_55ep_600', \n",
        "#                   save_name_prefix='/1000_training_',\n",
        "#                   colorspace='LAB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[32],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[19],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[42],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[43],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## must have length 8!\n",
        "images_to_visualize_indexes = [3,12,5,17,11,3,28,29]\n",
        "\n",
        "visualization_images = get_visualization_images(images_to_visualize_indexes)\n",
        "outputs = get_batch_outputs(visualization_images)\n",
        "plot_grid(visualization_images, outputs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
