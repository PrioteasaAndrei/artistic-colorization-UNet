{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6CH_vkV8IEs",
        "outputId": "ccc64d9e-60d8-4718-b4f5-0a4a870b01ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: mps\n"
          ]
        }
      ],
      "source": [
        "import dataset\n",
        "from visualize import *\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "from skimage import color\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
        "                else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /Users/matteom/miniconda3/envs/torch/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ],
      "source": [
        "import lpips\n",
        "perceptual_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "# d = loss_fn.forward(im0,im1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([4, 128])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class Adain_Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_dim):\n",
        "        super(Adain_Encoder, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, out_dim, kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = torch.relu(self.conv4(x))\n",
        "        x = torch.relu(self.conv5(x))\n",
        "        \n",
        "        # Global average pooling\n",
        "        x = self.global_avg_pool(x)        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# Define input and output dimensions\n",
        "in_channels = 3  # number of input channels\n",
        "out_dim = 128  # size of the output representation\n",
        "\n",
        "# Create an instance of the ConvNet\n",
        "model = Adain_Encoder(in_channels, out_dim)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(4, in_channels, 256, 256)\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([4, 256, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    \n",
        "    def __init__(self, style_dim, channels):\n",
        "        super(AdaIN, self).__init__()\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
        "        self.style_scale_transform = nn.Linear(style_dim, channels)\n",
        "        self.style_shift_transform = nn.Linear(style_dim, channels)\n",
        "\n",
        "        ## to ensure they learn different stuff | How tho?\n",
        "        init.normal_(self.style_scale_transform.weight, mean=1.0, std=0.02)\n",
        "        init.normal_(self.style_shift_transform.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        self.style_scale_transform.bias.data.fill_(1)  # Initialize scale to 1\n",
        "        self.style_shift_transform.bias.data.fill_(0)  # Initialize shift to 0\n",
        "\n",
        "    def forward(self, x, style):\n",
        "        '''\n",
        "        x - feature maps from the unet\n",
        "        y - learned (jointly) from encoder\n",
        "\n",
        "        return:\n",
        "        same size as x\n",
        "        '''\n",
        "        # Normalize the input feature map\n",
        "        normalized = self.instance_norm(x)\n",
        "        \n",
        "        # Extract style scale and shift parameters from the style vector\n",
        "        scale = self.style_scale_transform(style)[:, :, None, None]\n",
        "        shift = self.style_shift_transform(style)[:, :, None, None]\n",
        "        \n",
        "        # Apply scale and shift to the normalized feature map\n",
        "        transformed = scale * normalized + shift\n",
        "        \n",
        "        return transformed\n",
        "\n",
        "# Example usage:\n",
        "# Define the dimensions\n",
        "style_dim = 128  # dimensionality of the style vector\n",
        "channels = 256    # number of channels in the feature map\n",
        "\n",
        "# Create an instance of AdaIN\n",
        "adain = AdaIN(style_dim, channels)\n",
        "\n",
        "# Generate random input feature map and style vector\n",
        "x = torch.randn(4, channels, 64, 64)  # Example input feature map\n",
        "style = torch.randn(1, style_dim)     # Example style vector\n",
        "\n",
        "# Apply AdaIN\n",
        "\n",
        "output = adain(x, style)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse_loss_fn = nn.MSELoss()\n",
        "def perceptual_and_MSE_loss(reproduced_image, original_image):\n",
        "    '''\n",
        "    reproduced_image: output of the model\n",
        "    original_image: ground truth\n",
        "    '''\n",
        "    # Perceptual loss\n",
        "    perceptual_loss = perceptual_loss_fn(reproduced_image, original_image).mean()\n",
        "    \n",
        "    # MSE loss\n",
        "    mse_loss = mse_loss_fn(reproduced_image, original_image)\n",
        "    \n",
        "    return perceptual_loss + mse_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xq2e0mN_8IEw"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_C = 3, out_C=3, style_dim=128):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_C, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.maxpool_1to2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.maxpool_2to3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.maxpool_3to4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "        self.maxpool_4to5 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.conv_transpose_5to6 = nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_5to6 = nn.Conv2d(1024, 512, 1)\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.conv_transpose_6to7 = nn.ConvTranspose2d(256, 256, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_6to7 = nn.Conv2d(512, 256, 1)\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.conv_transpose_7to8 = nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_7to8 = nn.Conv2d(256, 128, 1)\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv_transpose8to9 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)\n",
        "        self.conv1d_fusing_8to9 = nn.Conv2d(128, 64, 1)\n",
        "        self.conv9 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, padding=1),      # Simmetry broken here: keeps being 64 (from paper)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.conv10 = nn.Conv2d(64, out_C, 1)\n",
        "\n",
        "        '''\n",
        "        Conv1 shape torch.Size([16, 64, 128, 128])\n",
        "        Conv2 shape torch.Size([16, 128, 64, 64])\n",
        "        Conv3 shape torch.Size([16, 256, 32, 32])\n",
        "        Conv4 shape torch.Size([16, 512, 16, 16])\n",
        "        Conv5 shape torch.Size([16, 512, 8, 8])\n",
        "        '''\n",
        "\n",
        "        self.encoder_adain1 = AdaIN(128, 64)\n",
        "        self.encoder_adain2 = AdaIN(128, 128)\n",
        "        self.encoder_adain3 = AdaIN(128, 256)\n",
        "        self.encoder_adain4 = AdaIN(128, 512)\n",
        "        self.encoder_adain5 = AdaIN(128, 512)\n",
        "\n",
        "        self.decoder_adain4 = AdaIN(128, 512)\n",
        "        self.decoder_adain3 = AdaIN(128, 256)\n",
        "        self.decoder_adain2 = AdaIN(128, 128)\n",
        "        self.decoder_adain1 = AdaIN(128, 64)\n",
        "\n",
        "        if out_C == 3:\n",
        "            self.style_encoder = Adain_Encoder(in_channels=3, out_dim=128)\n",
        "        elif out_C == 2:\n",
        "            self.style_encoder = Adain_Encoder(in_channels=2, out_dim=128)\n",
        "\n",
        "\n",
        "    def forward(self,x,style_image):\n",
        "        x=x.to(device)\n",
        "        style_image = style_image.to(device)\n",
        "\n",
        "        style = self.style_encoder(style_image).to(device)\n",
        "\n",
        "        # Encoder\n",
        "        conv1 = self.conv1(x)\n",
        "        conv1 = self.encoder_adain1(x,style) ## AdaIN\n",
        "\n",
        "        maxpooled_1to2 = self.maxpool_1to2(conv1)\n",
        "        conv2 = self.conv2(maxpooled_1to2)\n",
        "        conv2 = self.encoder_adain2(conv2,style) ## AdaIN\n",
        "\n",
        "        maxpooled_2to3 = self.maxpool_2to3(conv2)\n",
        "        conv3 = self.conv3(maxpooled_2to3)\n",
        "        conv3 = self.encoder_adain3(conv3,style) ## AdaIN\n",
        "\n",
        "        maxpooled_3to4 = self.maxpool_3to4(conv3)\n",
        "        conv4 = self.conv4(maxpooled_3to4)\n",
        "        conv4 = self.encoder_adain4(conv4,style) ## AdaIN\n",
        "\n",
        "        maxpooled_4to5 = self.maxpool_4to5(conv4)\n",
        "        conv5 = self.conv5(maxpooled_4to5)\n",
        "        conv5 = self.encoder_adain5(conv5,style) ## AdaIN\n",
        "        # Decoder\n",
        "        concatenation_5to6 = torch.cat((conv4,self.decoder_adain4(self.conv_transpose_5to6(conv5),style)),1)\n",
        "        # concatenation_5to6 = torch.cat((conv4,self.conv_transpose_5to6(conv5)),1)\n",
        "        skip_fusion_5to6 = self.conv1d_fusing_5to6(concatenation_5to6)\n",
        "        conv6 = self.conv6(skip_fusion_5to6)\n",
        "\n",
        "        concatenation_6to7 = torch.cat((conv3, self.decoder_adain3(self.conv_transpose_6to7(conv6),style)),1)\n",
        "        # concatenation_6to7 = torch.cat((conv3, self.conv_transpose_6to7(conv6)),1)\n",
        "        skip_fusion_6to7 = self.conv1d_fusing_6to7(concatenation_6to7)\n",
        "        conv7 = self.conv7(skip_fusion_6to7)\n",
        "\n",
        "        concatenation_7to8 = torch.cat((conv2, self.decoder_adain2(self.conv_transpose_7to8(conv7),style)),1)\n",
        "        # concatenation_7to8 = torch.cat((conv2, self.conv_transpose_7to8(conv7)),1)\n",
        "        skip_fusion_7to8 = self.conv1d_fusing_7to8(concatenation_7to8)\n",
        "        conv8 = self.conv8(skip_fusion_7to8)\n",
        "\n",
        "        concatenation_8_to9 = torch.cat((conv1, self.decoder_adain1(self.conv_transpose8to9(conv8),style)),1)\n",
        "        # concatenation_8_to9 = torch.cat((conv1, self.conv_transpose8to9(conv8)),1)\n",
        "        skip_fusion_8to9 = self.conv1d_fusing_8to9(concatenation_8_to9)\n",
        "        conv9 = self.conv9(skip_fusion_8to9)\n",
        "\n",
        "        output = self.conv10(conv9)\n",
        "        return output\n",
        "    \n",
        "    def style_loss(self, reproduced_image, style_image):\n",
        "        encoded_style = self.style_encoder(style_image)\n",
        "        encoded_style_of_reproduced = self.style_encoder(reproduced_image)\n",
        "        return mse_loss_fn(encoded_style, encoded_style_of_reproduced)\n",
        "    \n",
        "    def colorization_plus_style_loss(self, reproduced_image, original_image, style_image, alpha=0.5):\n",
        "        colorization_loss = perceptual_and_MSE_loss(reproduced_image, original_image)\n",
        "        style_loss = self.style_loss(reproduced_image, style_image)\n",
        "        return alpha * colorization_loss + (1 - alpha) * style_loss, colorization_loss, style_loss\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, \n",
        "                    epochs=54, \n",
        "                    lr=0.0001, \n",
        "                    loss_balance=0.5,\n",
        "                    optimizer=torch.optim.Adam, \n",
        "                    verbose=True, \n",
        "                    save_path = \"./model_storage/\", \n",
        "                    save_name_prefix='/',\n",
        "                    colorspace='RGB'):\n",
        "\n",
        "        '''\n",
        "        save_path = folder for saving results\n",
        "        save_name_prefix = prefix for saved files to mark experiments (start with '/')\n",
        "\n",
        "\n",
        "        '''\n",
        "\n",
        "\n",
        "        loss_df = pd.DataFrame(['training_loss', 'validation_loss'])\n",
        "        self.to(device)\n",
        "        self.train()\n",
        "        optimizer = optimizer(self.parameters(), lr=lr)\n",
        "\n",
        "        #for saving progress\n",
        "        best_val_loss = 99999\n",
        "        loss_archive = {\"training\": {\"combined\": [],\"colorization\":[],\"style\":[]}, \"validation\": {\"combined\": [],\"colorization\":[],\"style\":[]}}\n",
        "\n",
        "        mb = master_bar(range(epochs))\n",
        "        for epoch in mb:\n",
        "            training_combined_loss = 0\n",
        "            training_colorizaiton_loss = 0\n",
        "            training_style_loss = 0\n",
        "            for i, batch_data in progress_bar(enumerate(train_loader), total=len(train_loader), parent=mb):\n",
        "                # Input grayscale image\n",
        "                image = batch_data['grayscale_image']\n",
        "                image = image.to(device)\n",
        "                # Input style image\n",
        "                input_encoder = batch_data['image']\n",
        "                input_encoder = input_encoder.to(device)\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                reproduced_image = self(image,input_encoder)\n",
        "                reproduced_image = reproduced_image.to(device)\n",
        "                \n",
        "                if colorspace == 'RGB':\n",
        "                    combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(reproduced_image, batch_data['image'].to(device), input_encoder, alpha=loss_balance)\n",
        "                elif colorspace == 'LAB':\n",
        "                    combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(reproduced_image, batch_data['image'][:,1:,:,:].to(device), input_encoder, alpha=loss_balance)\n",
        "                training_combined_loss += combined_loss.item()\n",
        "                training_colorizaiton_loss += colorization_loss.item()\n",
        "                training_style_loss += style_loss.item()\n",
        "                combined_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            \n",
        "            #average training loss and append to achive\n",
        "            training_combined_loss /= len(train_loader)\n",
        "            loss_archive[\"training\"][\"combined\"].append(training_combined_loss)\n",
        "            training_colorizaiton_loss /= len(train_loader)\n",
        "            loss_archive[\"training\"][\"colorization\"].append(training_colorizaiton_loss)\n",
        "            training_style_loss /= len(train_loader)\n",
        "            loss_archive[\"training\"][\"style\"].append(training_style_loss)\n",
        "            \n",
        "\n",
        "            if (epoch)%5 == 0 or (epoch + 1) == epochs:\n",
        "                validation_combined_loss = 0\n",
        "                validation_colorizaiton_loss = 0\n",
        "                validation_style_loss = 0\n",
        "                with torch.no_grad():  # Disable gradient calculation for validation\n",
        "                    for val_data in val_loader:\n",
        "                        # Input grayscale image\n",
        "                        val_image = val_data['grayscale_image']\n",
        "                        val_image = val_image.to(device)\n",
        "                        # Input style image\n",
        "                        input_encoder = val_data['image']\n",
        "                        input_encoder = input_encoder.to(device)\n",
        "                        # Forward pass\n",
        "                        val_reproduced_image = self(val_image,input_encoder)\n",
        "                        val_reproduced_image = val_reproduced_image.to(device)\n",
        "                        if colorspace == 'RGB':\n",
        "                            combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(val_reproduced_image, val_data['image'].to(device),input_encoder,loss_balance)\n",
        "                        elif colorspace == 'LAB':\n",
        "                            combined_loss, colorization_loss, style_loss = self.colorization_plus_style_loss(val_reproduced_image, val_data['image'][:,1:,:,:].to(device),input_encoder,loss_balance)\n",
        "                        validation_combined_loss += combined_loss.item()\n",
        "                        validation_colorizaiton_loss += colorization_loss.item()\n",
        "                        validation_style_loss += style_loss.item()\n",
        "                       \n",
        "                validation_combined_loss /= len(val_loader)\n",
        "                loss_archive[\"validation\"][\"combined\"].append(validation_combined_loss)\n",
        "                validation_colorizaiton_loss /= len(val_loader)\n",
        "                loss_archive[\"validation\"][\"colorization\"].append(validation_colorizaiton_loss)\n",
        "                validation_style_loss /= len(val_loader)\n",
        "                loss_archive[\"validation\"][\"style\"].append(validation_style_loss)\n",
        "                \n",
        "\n",
        "                if verbose:\n",
        "                  print(f\"Epoch {epoch}, training_loss = {training_combined_loss}\")\n",
        "                  print(f\"Epoch {epoch}: validation loss = {validation_combined_loss}\")\n",
        "\n",
        "\n",
        "                if validation_combined_loss < best_val_loss:  # Update best validation loss and save checkpoint if best model\n",
        "                  state_diction = self.state_dict()\n",
        "                  best_val_loss = validation_combined_loss\n",
        "                  for key in state_diction.keys():\n",
        "                      state_diction[key] = state_diction[key].to(torch.device('cpu'))\n",
        "                  torch.save(state_diction, (save_path+save_name_prefix+f\"_best_model.pth.tar\"))\n",
        "\n",
        "                #construct df with losses\n",
        "                loss_df = pd.DataFrame({'epoch': range(0,epoch+1),  \n",
        "                                        'training_combined_loss':loss_archive['training'][\"combined\"], \n",
        "                                        \"training_colorization_loss\": loss_archive['training'][\"colorization\"], \n",
        "                                        \"training_style_loss\": loss_archive['training'][\"style\"],\n",
        "                                        'validation_combined_loss': loss_archive['validation'][\"combined\"],\n",
        "                                        \"validation_colorization_loss\": loss_archive['validation'][\"colorization\"],\n",
        "                                        \"validation_style_loss\": loss_archive['validation'][\"style\"]})\n",
        "                loss_df.to_csv(save_path+save_name_prefix+\"_loss.csv\", index=False)\n",
        "                # plot losses\n",
        "                plt.plot(loss_df['epoch'], loss_df['training_combined_loss'], label='Training Loss')\n",
        "                if loss_balance<0.99:\n",
        "                    plt.plot(loss_df[\"epoch\"], loss_df[\"training_colorization_loss\"], label=\"Training Colorization Loss\",linestyle='dashed')\n",
        "                    plt.plot(loss_df[\"epoch\"], loss_df[\"training_style_loss\"], label=\"Training Style Loss\",linestyle='dotted')\n",
        "\n",
        "                plt.plot(loss_df['epoch'], loss_df['validation_combined_loss'].interpolate(method='linear'), label='Validation Loss')\n",
        "                if loss_balance<0.99:\n",
        "                    plt.plot(loss_df[\"epoch\"], loss_df[\"validation_colorization_loss\"].interpolate(method='linear'), label=\"Validation Colorization Loss\",linestyle='dashed')\n",
        "                    plt.plot(loss_df[\"epoch\"], loss_df[\"validation_style_loss\"].interpolate(method='linear'), label=\"Validation Style Loss\",linestyle='dotted')\n",
        "\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.title('Training and Validation Loss')\n",
        "                plt.legend()\n",
        "\n",
        "                # Save plot as .png file\n",
        "                plt.savefig(save_path+save_name_prefix+'_loss_plot.png')\n",
        "                plt.clf()\n",
        "            else:\n",
        "                loss_archive[\"validation\"][\"combined\"].append(np.nan)\n",
        "                loss_archive[\"validation\"][\"colorization\"].append(np.nan)\n",
        "                loss_archive[\"validation\"][\"style\"].append(np.nan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_from_checkpoint(checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        # Load the checkpoint\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        \n",
        "        # Create model architecture\n",
        "        # For example, if your model class is named MyModel:\n",
        "        model = UNet(in_C=1, out_C=3)\n",
        "        \n",
        "        # Load state_dict into the model\n",
        "        model.load_state_dict(checkpoint)\n",
        "        \n",
        "        # Optionally, load other elements from the checkpoint such as optimizer state, etc.\n",
        "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        \n",
        "        print(\"Model loaded successfully from checkpoint.\")\n",
        "        return model.to(device)\n",
        "    else:\n",
        "        print(\"Checkpoint file does not exist.\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colorization from RGB color space. \n",
        "\n",
        "Input is a HxW grayscale image.\n",
        "Output is a 3xHxW RGB image.\n",
        "\n",
        "No std and mean normalization is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully\n",
            "Data loader prepared successfully\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nSince the model is trained against 3XHxW RGB images I expect the order of the channels in the ouput to be the same\\n'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TRAIN_SIZE = 5000\n",
        "VAL_SIZE = 500\n",
        "BATCH_SIZE = 12\n",
        "\n",
        "train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace='RGB',resolution=(128,128))\n",
        "train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "'''\n",
        "Since the model is trained against 3XHxW RGB images I expect the order of the channels in the ouput to be the same\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "⚠️ To use only colorization loss (former implementation) set loss_balance to 1.0\n",
        "\n",
        "Indeed, the new combined loss is defined as:\n",
        "\n",
        "$combined\\_loss = \\alpha * colorization\\_loss + (1-\\alpha)*style\\_loss$\n",
        "\n",
        "Note: colorization_loss is perceptual+MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is:  mps\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='37' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      37.00% [37/100 4:30:19&lt;7:40:16]\n",
              "    </div>\n",
              "    \n",
              "\n",
              "\n",
              "    <div>\n",
              "      <progress value='125' class='' max='411' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      30.41% [125/411 01:59&lt;04:34]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(in_C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_balance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcombined_losses_experiments/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_name_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/UNet-AdaIN_trsize5000_valsize500\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     state_diction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m state_diction\u001b[38;5;241m.\u001b[39mkeys():\n",
            "Cell \u001b[0;32mIn[16], line 243\u001b[0m, in \u001b[0;36mUNet.train_model\u001b[0;34m(self, train_loader, val_loader, epochs, lr, loss_balance, optimizer, verbose, save_path, save_name_prefix, colorspace)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    242\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 243\u001b[0m reproduced_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_encoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m reproduced_image \u001b[38;5;241m=\u001b[39m reproduced_image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m colorspace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[16], line 177\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x, style_image)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# concatenation_6to7 = torch.cat((conv3, self.conv_transpose_6to7(conv6)),1)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m skip_fusion_6to7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d_fusing_6to7(concatenation_6to7)\n\u001b[0;32m--> 177\u001b[0m conv7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv7\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskip_fusion_6to7\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m concatenation_7to8 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((conv2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_adain2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_transpose_7to8(conv7),style)),\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# concatenation_7to8 = torch.cat((conv2, self.conv_transpose_7to8(conv7)),1)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Device is: \", device)\n",
        "model = UNet(in_C=1, out_C=3).to(device)\n",
        "\n",
        "if TRAIN:\n",
        "    model.train_model(train_loader=train_loader, val_loader=validation_loader, epochs=100, lr=0.0001, loss_balance=1.0, optimizer=torch.optim.Adam, verbose=False, save_path= 'combined_losses_experiments/', save_name_prefix='/UNet-AdaIN_trsize5000_valsize500')\n",
        "    state_diction = model.state_dict()\n",
        "\n",
        "    for key in state_diction.keys():\n",
        "        state_diction[key] = state_diction[key].to(torch.device('cpu'))\n",
        "\n",
        "    torch.save(state_diction, \"UNet-AdaIN_trsize5000_valsize500_combined_losses.pth.tar\")\n",
        "else:\n",
        "    model = load_model_from_checkpoint(\"combined_losses_experiments/UNet-AdaIN_trsize5000_valsize500_best.pth.tar\")\n",
        "    model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Style analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_data_list = list(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_encoder_style = training_data_list[38]['image']\n",
        "\n",
        "## plot reference style\n",
        "plot_both(input_encoder_style.permute(1,2,0).numpy(),input_encoder_style.permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## style is the image itself\n",
        "entry = training_data_list[39]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = entry['image']\n",
        "\n",
        "output = model(input_unet.unsqueeze(0).to(device), input_encoder.unsqueeze(0).to(device))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "# plot_both(input_encoder.permute(1,2,0).numpy(),output.permute(1,2,0).numpy(),save_name=\"results/butterfly.png\")\n",
        "\n",
        "plot_both(input_encoder.permute(1,2,0).numpy(),output.permute(1,2,0).numpy(),save_name=\"results_combined_loss/leopard_no_style.png\")\n",
        "\n",
        "# plot_both(input_encoder.permute(1,2,0).numpy(),output.permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## style is the image itself\n",
        "entry = training_data_list[32]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = entry['image']\n",
        "\n",
        "output = model(input_unet.unsqueeze(0).to(device), input_encoder.unsqueeze(0).to(device))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "plot_both(entry['image'].permute(1,2,0).numpy(),output.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## style is the style image\n",
        "entry = training_data_list[19]\n",
        "input_unet = entry['grayscale_image']\n",
        "input_encoder = input_encoder_style\n",
        "input_encoder = entry['image']\n",
        "output = model(input_unet.unsqueeze(0).to(device), input_encoder.unsqueeze(0).to(device))\n",
        "output = output.detach().cpu().squeeze(0)\n",
        "plot_both(entry['image'].permute(1,2,0).numpy(),output.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reference_style = training_data_list[39]['image']\n",
        "plot_both(reference_style.permute(1,2,0).numpy(),reference_style.permute(1,2,0).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colorization from LAB color space. \n",
        "\n",
        "Input is a HxW grayscale image (the l channel).\n",
        "Output is 2xHxW representing the ab channels.\n",
        "\n",
        "No std and mean normalization is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAIN_SIZE = 600\n",
        "# VAL_SIZE = 150\n",
        "# BATCH_SIZE = 16\n",
        "\n",
        "# train_data, validation_data = dataset.prepare_dataset(train_size=TRAIN_SIZE, test_size=VAL_SIZE, batch_size=BATCH_SIZE,colorspace='LAB')\n",
        "# train_loader, validation_loader = dataset.prepare_dataloader(train_data, validation_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "# '''\n",
        "# Since the model is trained against 3XHxW RGB images I expect the order of the channels in the ouput to be the same\n",
        "# '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = UNet(in_C=1, out_C=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.train_model(train_loader=train_loader, val_loader=validation_loader, \n",
        "#                   epochs=55, \n",
        "#                   lr=0.0001, \n",
        "#                   loss_fn=nn.MSELoss(), \n",
        "#                   optimizer=torch.optim.Adam, \n",
        "#                   verbose=True, \n",
        "#                   save_path= 'experiment_results/LAB_55ep_600', \n",
        "#                   save_name_prefix='/1000_training_',\n",
        "#                   colorspace='LAB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[32],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[19],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[42],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize(list(train_data)[43],model,lab=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## must have length 8!\n",
        "images_to_visualize_indexes = [3,12,5,17,11,3,28,29]\n",
        "\n",
        "visualization_images = get_visualization_images(images_to_visualize_indexes)\n",
        "outputs = get_batch_outputs(visualization_images)\n",
        "plot_grid(visualization_images, outputs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
